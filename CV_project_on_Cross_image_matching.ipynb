{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palu001/Interactive-Graphics-Project/blob/main/CV_project_on_Cross_image_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "j68I0TkRDOxc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y9VSTxIBhU6i"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gdown\n",
        "!pip install rarfile\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "qs909SjRPoJ4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wHBhZ74x6XwB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets opencv-contrib-python opencv-python ipywidgets scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FNDG_MB82sw",
        "outputId": "676c5474-98aa-4263-ce8d-3ea2e20fc80b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pytorch_lightning as pl\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import random_split\n",
        "import logging\n",
        "from collections import OrderedDict\n",
        "import torchvision\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning import Trainer\n",
        "from torch import optim\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from pytorch_lightning.callbacks.progress import RichProgressBar\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pE07XccPTCFg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#University Dataset da cambiare"
      ],
      "metadata": {
        "id": "FTH52kmbCo0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O file.zip 'https://drive.google.com/file/d/17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_/view?usp=sharing' #Download dataset\n"
      ],
      "metadata": {
        "id": "jLLcFzdGhIRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Percorso del file zip\n",
        "file_zip = \"dataset.zip\"\n",
        "\n",
        "# Percorso di destinazione per l'estrazione\n",
        "percorso_estrazione = \"/content\"\n",
        "\n",
        "# Estrai il file zip\n",
        "with zipfile.ZipFile(file_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(percorso_estrazione)\n",
        "\n",
        "# Mostra i file estratti\n",
        "print(\"File estratti in:\", percorso_estrazione)\n",
        "print(\"Elenco dei file estratti:\")\n",
        "for file in os.listdir(percorso_estrazione):\n",
        "    print(file)"
      ],
      "metadata": {
        "id": "iRgfju7XVnpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CVUSA Dataset"
      ],
      "metadata": {
        "id": "zN_O1FwaDMw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load directly without drive"
      ],
      "metadata": {
        "id": "9NEGQqWT7S__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "!pip install rarfile\n",
        "!gdown --id 17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CFPXSjLQSQFx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ff5bc478-e152-4d36-f6cd-41dd62f6584d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n!pip install rarfile\\n!gdown --id 17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import rarfile\n",
        "# import os\n",
        "\n",
        "# # Percorso del file RAR\n",
        "# file_rar = \"/content/drive/MyDrive/CV/CVUSA_subset.rar\"\n",
        "\n",
        "# # Directory di destinazione per l'estrazione\n",
        "# extract_path = \"/content/CVUSA_subset\"\n",
        "\n",
        "# # Estrarre il file RAR\n",
        "# with rarfile.RarFile(file_rar, 'r') as rar:\n",
        "#     # Crea la directory di destinazione se non esiste\n",
        "#     os.makedirs(extract_path, exist_ok=True)\n",
        "#     # Estrai tutto nella directory di destinazione\n",
        "#     rar.extractall(extract_path)\n",
        "\n",
        "# print(\"Estrazione completata.\")"
      ],
      "metadata": {
        "id": "HKdcsd0_DMJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP4np_gE480R",
        "outputId": "035cccc0-13b0-4679-db61-0cbaef963a51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorsi delle directory\n",
        "#Francesco\n",
        "bingmap_dir = '/content/drive/MyDrive/CVUSA/bingmap'\n",
        "streetview_dir = '/content/drive/MyDrive/CVUSA/streetview'\n",
        "cvusa_dir = '/content/drive/MyDrive/CVUSA'\n",
        "#Simone\n",
        "bingmap_dir = '/content/drive/MyDrive/CV/CVUSA_subset/bingmap'\n",
        "streetview_dir = '/content/drive/MyDrive/CV/CVUSA_subset/streetview'\n",
        "cvusa_dir = '/content/drive/MyDrive/CV/CVUSA_subset'\n",
        "\n",
        "# Leggi i file dalle directory\n",
        "bingmap_files = sorted(os.listdir(bingmap_dir))\n",
        "streetview_files = sorted(os.listdir(streetview_dir))\n",
        "\n",
        "# Assicurati che ci sia una corrispondenza uno a uno\n",
        "if len(bingmap_files) != len(streetview_files):\n",
        "    raise ValueError(\"Le directory non contengono lo stesso numero di file\")\n",
        "\n",
        "# Percorso del file CSV\n",
        "csv_path = '/content/file_paths.csv'\n",
        "\n",
        "# Scrivi i percorsi dei file nel CSV\n",
        "with open(csv_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for bingmap_file, streetview_file in zip(bingmap_files, streetview_files):\n",
        "        bingmap_path = os.path.join(bingmap_dir, bingmap_file)\n",
        "        streetview_path = os.path.join(streetview_dir, streetview_file)\n",
        "        writer.writerow([bingmap_path, streetview_path])\n",
        "\n",
        "print(f\"CSV creato con successo in {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a5caYWNdJZg",
        "outputId": "ba2c5d89-2454-4787-8fe9-52508e818adf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV creato con successo in /content/file_paths.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "XZYwOT8J-zJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CVUSADatasetPath(Dataset):\n",
        "    def __init__(self, cvusa_dir, directory):\n",
        "        self.directory = directory\n",
        "        self.cvusa_dir = cvusa_dir\n",
        "        self.aerial_paths = []\n",
        "        self.ground_paths = []\n",
        "\n",
        "        with open(self.directory, mode='r') as file:\n",
        "          csv_reader = csv.reader(file, delimiter=',')\n",
        "          for row in csv_reader:\n",
        "              aerial_path = os.path.join(self.cvusa_dir, row[0])\n",
        "              ground_path = os.path.join(self.cvusa_dir, row[1])\n",
        "              if os.path.exists(aerial_path) and os.path.exists(ground_path):\n",
        "                  self.aerial_paths.append(aerial_path)\n",
        "                  self.ground_paths.append(ground_path)\n",
        "              else:\n",
        "                  print(f\"File not found: {aerial_path} or {ground_path}\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.aerial_paths) # any image path is good\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.ground_paths[idx], self.aerial_paths[idx]"
      ],
      "metadata": {
        "id": "xKYeh_V7-7Md"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CVUSADataset(pl.LightningDataModule):\n",
        "    def __init__(self, cvusa_dir, directory, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.cvusa_dir = cvusa_dir\n",
        "        self.directory = directory\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.dataset = CVUSADatasetPath(self.cvusa_dir, self.directory)\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(self.dataset, [0.8, 0.1, 0.1])\n",
        "\n",
        "    def transform(self, set_type):\n",
        "      transform = None\n",
        "      if set_type == 'train':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))\n",
        "        ])\n",
        "      else:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))\n",
        "        ])\n",
        "      return transform\n",
        "\n",
        "\n",
        "    def load_images(self, paths, set_type):\n",
        "      images = []\n",
        "      idx = []\n",
        "      for i in range(len(paths)):\n",
        "        img = cv2.imread(paths[i])\n",
        "        img = Image.fromarray(img)\n",
        "        img = self.transform(set_type)(img)\n",
        "        images.append(img)\n",
        "        idx.append(paths[i])\n",
        "      return torch.stack(images)\n",
        "\n",
        "    def collate_fn_train(self, batch):\n",
        "      return self.collate_fn(batch, 'train')\n",
        "\n",
        "    def collate_fn_val(self, batch):\n",
        "      return self.collate_fn(batch, 'val')\n",
        "\n",
        "    def collate_fn(self, batch, set_type):\n",
        "      ground_paths, aerial_paths = zip(*batch)\n",
        "      ground_images= self.load_images(ground_paths, set_type)\n",
        "      aerial_images= self.load_images(aerial_paths, set_type)\n",
        "      return ground_images, aerial_images\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn = self.collate_fn_train, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, collate_fn = self.collate_fn_val, num_workers=2)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn = self.collate_fn_val, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "sEgrFQwAIIo1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Siamese Branches"
      ],
      "metadata": {
        "id": "208opFqWJDYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###VGG"
      ],
      "metadata": {
        "id": "YImTHx7zNk7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "class SiameseNetworkVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetworkVGG, self).__init__()\n",
        "\n",
        "        # Load the pretrained VGG16 model\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "        self.vgg16_features = vgg16.features\n",
        "        # print(\"features: \", vgg16.features)  #feature vgg\n",
        "\n",
        "        # Freeze the parameters of all layers except the last block\n",
        "        for param in self.vgg16_features[:24].parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(512 * 7 * 7, 512)  # Adjust input size based on VGG16 output\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        # print(\"x prima di vgg: \", x.size())\n",
        "        x = self.vgg16_features(x)\n",
        "        # print(\"x: \", x.size())\n",
        "        x = F.adaptive_avg_pool2d(x, (7, 7))\n",
        "        # print(\"x dopo avg_pool: \",x.size())\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        grd_global = self.forward_one(input1)\n",
        "        sat_global = self.forward_one(input2)\n",
        "        return grd_global, sat_global\n",
        "\n"
      ],
      "metadata": {
        "id": "-c2FAN8IJFMl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RESNET BRANCH"
      ],
      "metadata": {
        "id": "wdMTOVNGOdLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "class SiameseNetworkRESNET(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetworkRESNET, self).__init__()\n",
        "\n",
        "        # Load the pretrained ResNet-50 model with the best available weights\n",
        "        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "\n",
        "        # Extract the feature extraction part of ResNet-50 (remove the last two layers)\n",
        "        self.resnet_features = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "        # Optionally freeze some of the ResNet layers\n",
        "        for param in list(self.resnet_features.parameters())[:-3]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(2048 * 7 * 7, 512)  # Adjust input size based on ResNet-50 output\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.resnet_features(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (7, 7))  # Ensure the correct input size to the fc1 layer\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        grd_global = self.forward_one(input1)\n",
        "        sat_global = self.forward_one(input2)\n",
        "        return grd_global, sat_global\n"
      ],
      "metadata": {
        "id": "ReH4gbkjOfkm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TRANSFORMER BRANCH"
      ],
      "metadata": {
        "id": "n8LvyWcZT6Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "# model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "# inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "# outputs = model(**inputs)\n",
        "# last_hidden_states = outputs.last_hidden_state"
      ],
      "metadata": {
        "id": "4umH1jS9bKWz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "class SiameseNetworkVIT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetworkVIT, self).__init__()\n",
        "\n",
        "        self.processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        # self.model.train()\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(151296, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        print(\"x prima: \",x.size())\n",
        "        x = self.processor(images=x, return_tensors=\"pt\")\n",
        "        print(\"x dopo processor: \",x[\"pixel_values\"].size())\n",
        "        outputs = self.model(**x)\n",
        "        x = outputs.last_hidden_state\n",
        "        print(\"x last_hidden state: \",x.size())\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        print(\"x finale: \",x.size())\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        grd_global = self.forward_one(input1)\n",
        "        sat_global = self.forward_one(input2)\n",
        "        return grd_global , sat_global\n",
        "\n",
        "# Example usage:\n",
        "# model = SiameseNetwork()\n",
        "# output = model(input1, input2)\n"
      ],
      "metadata": {
        "id": "08iponz_T9Mh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GENERAL NETWORK"
      ],
      "metadata": {
        "id": "bgpcIFgJOgAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseNetworkLightning(pl.LightningModule):\n",
        "    def __init__(self, learning_rate=1e-3, branch = \"VGG\"):\n",
        "        super(SiameseNetworkLightning, self).__init__()\n",
        "        self.branch = branch\n",
        "        if branch == \"VGG\":\n",
        "          self.model = SiameseNetworkVGG()\n",
        "        elif branch == \"RESNET\":\n",
        "          self.model = SiameseNetworkRESNET()\n",
        "        elif branch == \"VIT\":\n",
        "          self.model = SiameseNetworkVIT()\n",
        "        else:\n",
        "          raise ValueError(\"Branch must be either 'VGG' or 'RESNET'\")\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_weight = 10.0\n",
        "\n",
        "    def loss(self,triplet_dist_g2s , triplet_dist_s2g , pair_n ):\n",
        "        # print(\"pair\", pair_n)\n",
        "        loss_g2s = torch.sum(torch.log(1 + torch.exp(triplet_dist_g2s * self.loss_weight))) / pair_n\n",
        "        loss_s2g = torch.sum(torch.log(1 + torch.exp(triplet_dist_s2g * self.loss_weight))) / pair_n\n",
        "        # print(\"loss_g2s: \",loss_g2s)\n",
        "        # print(\"loss_s2g: \",loss_s2g)\n",
        "        loss = (loss_g2s + loss_s2g) / 2.0\n",
        "        # print(\"loss: \", loss)\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validate(self, grd_descriptor, sat_descriptor):\n",
        "        accuracy = 0.0\n",
        "        data_amount = 0.0\n",
        "        dist_array = 2 - 2 * np.matmul(sat_descriptor.cpu(), np.transpose(grd_descriptor.cpu()))\n",
        "        top1_percent = int(dist_array.shape[0] * 0.01) + 1\n",
        "        for i in range(dist_array.shape[0]):\n",
        "            gt_dist = dist_array[i, i]\n",
        "            prediction = torch.sum(dist_array[:, i].cpu().lt(gt_dist.cpu()))\n",
        "            # print(\"la pred: \",prediction)\n",
        "            if prediction < top1_percent:\n",
        "                accuracy += 1.0\n",
        "            data_amount += 1.0\n",
        "        accuracy /= data_amount\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def validate_topk(self, dist_array, topK):\n",
        "        accuracy = 0.0\n",
        "        data_amount = 0.0\n",
        "\n",
        "        for i in range(dist_array.shape[0]):\n",
        "            gt_dist = dist_array[i, i]\n",
        "            prediction = np.sum(dist_array[i, :] < gt_dist)\n",
        "            if prediction < topK:\n",
        "                accuracy += 1.0\n",
        "            data_amount += 1.0\n",
        "        accuracy /= data_amount\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    #deve sempre essere uguale a quella definita in cvusadataset\n",
        "    def transform(self):\n",
        "      transform = transforms.Compose([\n",
        "          transforms.Resize((224, 224)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))\n",
        "      ])\n",
        "\n",
        "      return transform\n",
        "\n",
        "    def search(self, path, paths):\n",
        "        images_search = []\n",
        "        # Leggi e trasforma l'immagine di base\n",
        "        img_base = cv2.imread(path)\n",
        "        img_base = Image.fromarray(img_base)\n",
        "        img_base = self.transform()(img_base)\n",
        "        img_base = img_base.unsqueeze(0).to(self.device)  # Aggiungi dimensione batch e sposta sulla GPU\n",
        "\n",
        "        # Leggi e trasforma le immagini di ricerca\n",
        "        for i in range(len(paths)):\n",
        "            img = cv2.imread(paths[i])\n",
        "            img = Image.fromarray(img)\n",
        "            img = self.transform()(img)\n",
        "            img = img.unsqueeze(0).to(self.device)  # Aggiungi dimensione batch e sposta sulla GPU\n",
        "            images_search.append(img)\n",
        "\n",
        "        # Combina tutte le immagini di ricerca in un singolo tensore\n",
        "        images_search = torch.cat(images_search)\n",
        "\n",
        "        # Debug: Stampa per verificare le dimensioni dei tensori\n",
        "        print(f\"img_base shape: {img_base.shape}\")\n",
        "        print(f\"images_search shape: {images_search.shape}\")\n",
        "\n",
        "        # Passa le immagini attraverso il modello\n",
        "        print(1)\n",
        "        grd_global, sat_global = self.model(img_base, images_search)\n",
        "        grd_global = F.normalize(grd_global, dim=1)\n",
        "        sat_global = F.normalize(sat_global, dim=1)\n",
        "        dist_matrix = 2 - 2 * torch.matmul(sat_global, grd_global.t())\n",
        "\n",
        "\n",
        "        return dist_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # def search(self, grd_descriptor, sat_descriptor, grd_data , sat_data ,topK):\n",
        "\n",
        "    #     dist_array = 2 - 2 * np.matmul(sat_descriptor.cpu(), np.transpose(grd_descriptor.cpu()))\n",
        "    #     topk_indices = []\n",
        "\n",
        "    #     for i in range(dist_array.shape[0]):\n",
        "    #         gt_dist = dist_array[i, i]\n",
        "    #         distances = dist_array[i, :]\n",
        "    #         distances[i] = np.inf  # Escludi la distanza con se stesso\n",
        "\n",
        "    #         # Trova gli indici delle top-K immagini (con distanze minori di gt_dist)\n",
        "    #         topk_indices_i = np.argsort(distances)[:topK]\n",
        "    #         topk_indices.append(topk_indices_i)\n",
        "\n",
        "    #     print(\"topk indices: \",topk_indices)\n",
        "\n",
        "    #     for i in range(len(grd_data)):\n",
        "    #         print(\"indici: \",topk_indices[i])\n",
        "    #         print(\"immagine originale: \")\n",
        "    #         self.imshow(grd_data[i])\n",
        "    #         for j in range(topK):\n",
        "    #           print(j,\"più vicina\")\n",
        "    #           self.imshow(sat_data[topk_indices[i][j]])\n",
        "    #           print(\"ground della\",i,\" più vicina: \")\n",
        "    #           self.imshow(grd_data[topk_indices[i][j]])\n",
        "\n",
        "    #     return None\n",
        "\n",
        "\n",
        "    def imshow(self, img):\n",
        "        img = img / 2 + 0.5  # De-normalizzare\n",
        "        npimg = img.numpy()\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "    def forward(self, ground, aerial):\n",
        "        grd_global, sat_global = self.model(ground, aerial)\n",
        "        grd_global = F.normalize(grd_global, dim=1)\n",
        "        sat_global = F.normalize(sat_global, dim=1)\n",
        "        #print(\"grd_global: \", grd_global)\n",
        "        #print(\"sat_global: \", sat_global)\n",
        "\n",
        "        dist_matrix = 2 - 2 * torch.matmul(sat_global, grd_global.t())\n",
        "        print(\"dist_matrix: \",dist_matrix)\n",
        "        pos_dist = torch.diag(dist_matrix)\n",
        "\n",
        "        batch_size = sat_global.size(0)\n",
        "        print(\"batch_size: \",batch_size)\n",
        "\n",
        "        pair_n = batch_size * (batch_size - 1.0)\n",
        "\n",
        "        # ground to satellite\n",
        "        triplet_dist_g2s = pos_dist - dist_matrix\n",
        "\n",
        "        # satellite to ground\n",
        "        triplet_dist_s2g = pos_dist.unsqueeze(1) - dist_matrix\n",
        "\n",
        "        #print(\"triplet_dist_g2s: \", triplet_dist_g2s)\n",
        "        #print(\"triplet_dist_s2g: \", triplet_dist_s2g)\n",
        "\n",
        "        return triplet_dist_g2s, triplet_dist_s2g, pair_n, grd_global, sat_global\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      grd, sat = batch\n",
        "      # print(\"batch id: \", batch_idx)\n",
        "      dist_g2s , dist_s2g , pair_n , grd_global , sat_global = self.forward(grd ,sat)\n",
        "\n",
        "      loss_val = self.loss(dist_g2s , dist_s2g , pair_n)\n",
        "      self.log('train_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "      tqdm_dict = OrderedDict({\"loss_train\": loss_val})\n",
        "      output = {\"loss\": loss_val, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
        "      return output\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        grd, sat = batch\n",
        "        dist_g2s, dist_s2g, pair_n, grd_global , sat_global = self.forward(grd, sat)\n",
        "        loss_val = self.loss(dist_g2s, dist_s2g, pair_n)\n",
        "        # self.search(grd_global, sat_global, grd, sat, 2)\n",
        "        accuracy_val = self.validate(grd_global, sat_global)  # Calcola l'accuratezza\n",
        "        self.log('val_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        self.log('val_accuracy', accuracy_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        output= {\"val_loss\": loss_val, \"val_accuracy\": accuracy_val}\n",
        "        return output\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        grd, sat = batch\n",
        "        dist_g2s, dist_s2g, pair_n, grd_global, sat_global = self.forward(grd, sat)\n",
        "        loss_val = self.loss(dist_g2s, dist_s2g, pair_n)\n",
        "        accuracy_val = self.validate(grd_global, sat_global)  # Calcola l'accuratezza\n",
        "\n",
        "        self.log('test_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        self.log('test_accuracy', accuracy_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        return {\"test_loss\": loss_val, \"test_accuracy\": accuracy_val}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n"
      ],
      "metadata": {
        "id": "oabbPLlqT1gx"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/file_paths.csv\"\n",
        "batch_size = 8\n",
        "data_module = CVUSADataset(cvusa_dir, directory, batch_size)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # De-normalizzare\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Iterare attraverso il DataLoader e stampare i batch\n",
        "for batch_idx, (ground_images, aerial_images, idx_g, idx_a) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}\")\n",
        "    print(\"Ground images: \", idx_g[0])\n",
        "    imshow(ground_images[0])\n",
        "    print(\"Aerial images:\", idx_a[0])\n",
        "    imshow(aerial_images[0]) \"\"\"\n",
        "\n",
        "\n",
        "    # Limit the number of printed batches for readability\n",
        "model = SiameseNetworkLightning(branch=\"VGG\")\n",
        "trainer = Trainer(max_epochs=1)\n",
        "trainer.fit(model, data_module)\n",
        "#trainer.validate(model, data_module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843,
          "referenced_widgets": [
            "c5292d1d30d3441a8feba1c4303932d6",
            "f0a762484a71477b8194921a7e9c9adb",
            "26080088f6bc45e8be22d49f3aca300c",
            "cd0d39b103fa465784b9d7a2ee0eb650",
            "4835c22f40574b17a88c128fcbb0e6bf",
            "4bf53a74fff949c6b04414fe6e530a4d",
            "6eeb98aaee8247dd907952d052598793",
            "918a00e4b8c04d5ea4681b3c877a73fc",
            "fae79de488024e2c8b305f85779aa08d",
            "8d216f9f882c405fb81c069dc2bd0687",
            "d4651bb156104b29a213464cc1cd4143",
            "e98c2e05772e4863b73365128a9aa15b",
            "b69639b4b33a4c88b0c2cd537211056c",
            "c60798683684407c837ceddeb52cc7c5",
            "481edc1378154bc08dcdebdc99c56f42",
            "ae537f70181e44aaaeb8199facb96db8",
            "95ed057cd7d84c8dbf62d6a47c0d6189",
            "82f2187e09844187baf4dedb8ca2a7d8",
            "60340a0eeca144efb830825f012f2de4",
            "12be7ead9570488994a6a4b52221cc17",
            "54c36d5c1fe14e9dbda34b28519bbe95",
            "9441d7e850b84e64bec1cb235797d98c"
          ]
        },
        "id": "71PHcx1AV10F",
        "outputId": "c9ca0a27-4eb8-43e3-8357-3a3ee6e8a5ba"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type              | Params | Mode \n",
            "----------------------------------------------------\n",
            "0 | model | SiameseNetworkVGG | 27.7 M | train\n",
            "----------------------------------------------------\n",
            "20.1 M    Trainable params\n",
            "7.6 M     Non-trainable params\n",
            "27.7 M    Total params\n",
            "110.766   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5292d1d30d3441a8feba1c4303932d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dist_matrix:  tensor([[1.2687, 1.3740, 1.2305, 1.3097, 1.3603, 1.2938, 1.1968, 1.0981],\n",
            "        [1.2859, 1.3127, 1.2131, 1.2581, 1.2201, 1.1048, 1.3243, 1.1878],\n",
            "        [1.2636, 1.3268, 1.1789, 1.0153, 1.1874, 1.1869, 1.0516, 1.0405],\n",
            "        [1.2481, 1.2801, 1.0537, 1.3334, 1.1590, 1.1298, 1.1166, 0.9874],\n",
            "        [1.1508, 1.2278, 0.9546, 1.1848, 1.0319, 1.0495, 1.0598, 0.9782],\n",
            "        [1.4188, 1.3082, 1.1959, 1.1249, 1.2267, 1.1771, 1.2578, 1.1072],\n",
            "        [1.2252, 1.0838, 1.1301, 1.3126, 1.1771, 1.2846, 1.1179, 1.0646],\n",
            "        [1.2766, 1.2933, 1.2426, 1.4076, 1.3525, 1.3818, 1.3426, 1.1248]],\n",
            "       device='cuda:0')\n",
            "batch_size:  8\n",
            "dist_matrix:  tensor([[1.2510, 1.2730, 1.0795, 1.1339, 1.0683, 1.1781, 1.1473, 1.1036],\n",
            "        [1.2029, 1.3255, 1.1203, 1.2243, 1.2667, 1.2152, 1.2302, 1.1786],\n",
            "        [1.0457, 1.1401, 1.0293, 1.3776, 1.2946, 1.0490, 0.9378, 1.0856],\n",
            "        [1.2682, 1.1482, 1.1354, 1.1014, 1.1991, 1.2723, 1.2385, 1.2509],\n",
            "        [1.0506, 1.1789, 1.2503, 1.1485, 1.1446, 1.1965, 1.2942, 1.2976],\n",
            "        [1.1534, 1.3172, 1.0707, 1.2818, 1.0739, 1.3436, 1.1922, 1.2300],\n",
            "        [1.3397, 1.3987, 1.1571, 1.2905, 1.3809, 1.3222, 1.2437, 1.3456],\n",
            "        [1.0874, 1.3219, 1.1458, 1.1564, 1.2199, 1.1408, 1.2535, 1.5017]],\n",
            "       device='cuda:0')\n",
            "batch_size:  8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e98c2e05772e4863b73365128a9aa15b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dist_matrix:  tensor([[1.2213, 1.6463, 1.2737, 1.3697, 1.4204, 1.4349, 1.3334, 1.6045],\n",
            "        [1.1376, 1.3173, 1.0549, 1.1213, 1.2134, 1.0165, 1.1207, 1.1791],\n",
            "        [1.2820, 1.4419, 1.4091, 1.5079, 1.4088, 1.2958, 1.5127, 1.6278],\n",
            "        [1.1912, 1.2495, 1.2023, 1.0288, 1.1823, 1.1562, 1.4570, 1.4152],\n",
            "        [1.2353, 1.2510, 1.0708, 1.2022, 1.3861, 1.3284, 1.2460, 1.2219],\n",
            "        [1.1746, 1.1566, 1.3877, 1.1590, 1.3075, 1.2475, 1.3072, 1.1642],\n",
            "        [1.4984, 1.3796, 1.1606, 1.2470, 1.4001, 1.3456, 1.2948, 1.3843],\n",
            "        [1.1105, 1.4452, 1.3235, 1.2558, 1.1667, 1.3262, 1.3117, 1.3457]],\n",
            "       device='cuda:0', grad_fn=<RsubBackward1>)\n",
            "batch_size:  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.search(data_module.dataset[0][0],data_module.dataset[:3][1]))"
      ],
      "metadata": {
        "id": "0WlNZNpcyaXe",
        "outputId": "4052dd09-f4cd-443b-ca5b-8cf87e7c63d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img_base shape: torch.Size([1, 3, 224, 224])\n",
            "images_search shape: torch.Size([3, 3, 224, 224])\n",
            "1\n",
            "dist_matrix:  tensor([[0.2336],\n",
            "        [0.2759],\n",
            "        [0.2113]], device='cuda:0', grad_fn=<RsubBackward1>)\n",
            "batch_size:  3\n",
            "2\n",
            "grd_global: tensor([[ 0.0143, -0.0286, -0.0592, -0.0905, -0.0657,  0.1180,  0.0010,  0.0015,\n",
            "          0.0941,  0.0699,  0.0085, -0.0069,  0.1023,  0.0519, -0.0415,  0.1064,\n",
            "          0.0786, -0.0013,  0.0504,  0.0212,  0.0355,  0.0378, -0.0587,  0.1061,\n",
            "         -0.0414,  0.0352,  0.0350, -0.0374,  0.0449, -0.0021,  0.1326,  0.0592,\n",
            "         -0.0388, -0.0384,  0.0349,  0.1133, -0.0184,  0.0581,  0.1425, -0.0748,\n",
            "         -0.0724,  0.0550,  0.0169,  0.0306, -0.0581,  0.1122, -0.1419,  0.0339,\n",
            "          0.0661, -0.0924, -0.1435, -0.1297,  0.1046,  0.0939, -0.0051,  0.0181,\n",
            "          0.0271,  0.1172, -0.0081,  0.0231,  0.0813, -0.0327, -0.0274,  0.0554,\n",
            "         -0.0303, -0.0251, -0.0646,  0.0168, -0.0752, -0.0653, -0.1155, -0.0840,\n",
            "          0.0578, -0.0421, -0.0366, -0.0284,  0.0503, -0.0054, -0.0105,  0.0370,\n",
            "         -0.1208, -0.0614, -0.0582, -0.0248, -0.0291,  0.0523, -0.0129,  0.0337,\n",
            "          0.0419,  0.0014,  0.0673, -0.0481,  0.0488,  0.0426,  0.0508, -0.0270,\n",
            "          0.0466, -0.0138,  0.0330,  0.0402,  0.0108,  0.0246, -0.0339,  0.1406,\n",
            "         -0.0040, -0.0722, -0.0400, -0.0488,  0.0465, -0.0213, -0.0126,  0.1181,\n",
            "          0.0083, -0.0291, -0.0802, -0.0288,  0.0112, -0.0911, -0.0121, -0.0872,\n",
            "         -0.0371, -0.0416,  0.0282,  0.0074,  0.0051, -0.0053, -0.0205,  0.0163,\n",
            "          0.0796,  0.0692, -0.0268, -0.0646, -0.0220,  0.1175, -0.0179, -0.0167,\n",
            "         -0.0136,  0.0406,  0.1578, -0.0151,  0.0100,  0.0700,  0.1183,  0.0328,\n",
            "         -0.0469,  0.0589, -0.1154, -0.0129, -0.0646, -0.0263,  0.1363, -0.1232,\n",
            "         -0.0560, -0.1067,  0.1169, -0.0622,  0.0116,  0.0285,  0.0758, -0.0006,\n",
            "         -0.0892,  0.0665, -0.0568,  0.0242,  0.0083,  0.0590,  0.0147, -0.0947,\n",
            "          0.0358,  0.0559,  0.0696,  0.0327,  0.0946,  0.0806,  0.0388, -0.0619,\n",
            "         -0.1105,  0.1258, -0.0319,  0.0112,  0.0429, -0.0422,  0.0324,  0.0910,\n",
            "          0.0199, -0.0656,  0.0272, -0.0017,  0.1168,  0.0004,  0.0252, -0.0478,\n",
            "          0.0300, -0.0036,  0.0126, -0.0430, -0.0328,  0.0085,  0.0486, -0.0421,\n",
            "         -0.0168,  0.0126, -0.0227, -0.0337,  0.0863, -0.0938, -0.0164,  0.0219,\n",
            "          0.0579,  0.0255, -0.0249,  0.0724, -0.0990, -0.0294, -0.0060,  0.0849,\n",
            "         -0.0307, -0.0157,  0.0240,  0.0853, -0.0484,  0.1031, -0.0174,  0.0809,\n",
            "          0.0539,  0.0269,  0.0508,  0.0187, -0.1211,  0.1455,  0.0894,  0.0932,\n",
            "         -0.0762, -0.0321, -0.0267, -0.0175,  0.0322,  0.0390,  0.0910,  0.0054,\n",
            "         -0.0081,  0.0059,  0.0142,  0.0492, -0.1006, -0.0072, -0.1295,  0.0448,\n",
            "          0.0093,  0.0487, -0.0208,  0.0814, -0.0459,  0.0028,  0.0041, -0.0838]],\n",
            "       device='cuda:0', grad_fn=<DivBackward0>)\n",
            "sat_global: tensor([[-2.2186e-03, -4.1904e-02, -3.0070e-02, -7.5217e-02, -9.3187e-02,\n",
            "          1.3587e-01,  4.4309e-02,  2.0676e-02,  7.3051e-02,  6.5686e-02,\n",
            "          1.1432e-02, -9.3479e-03,  1.4146e-01,  8.3408e-02, -1.9202e-02,\n",
            "          9.4456e-02,  5.8546e-02, -1.0081e-02,  1.2372e-02,  5.8032e-02,\n",
            "          2.0702e-02,  3.9879e-02, -4.2197e-02,  8.4162e-02, -4.5615e-02,\n",
            "          5.0025e-02,  3.3904e-02, -1.0788e-03,  5.1697e-02, -1.9322e-02,\n",
            "          1.4801e-01,  3.2380e-02, -5.3440e-02, -6.7766e-02,  1.9638e-02,\n",
            "          7.5231e-02,  1.6101e-02,  4.3912e-02,  1.3500e-01, -6.9215e-02,\n",
            "         -5.0294e-02,  5.9273e-02, -3.6527e-03,  3.8251e-02, -7.6834e-02,\n",
            "          3.7678e-02, -1.1015e-01,  2.7828e-02,  1.0677e-01, -3.8323e-02,\n",
            "         -1.1143e-01, -1.1416e-01,  7.1389e-02,  8.1160e-02,  1.7749e-04,\n",
            "          6.3281e-02, -7.2341e-03,  1.0918e-01,  6.5763e-03,  4.6766e-02,\n",
            "          9.4458e-02, -2.6202e-02, -8.1345e-02,  3.5584e-02, -6.2383e-02,\n",
            "         -4.3754e-02, -3.6079e-02,  4.7756e-02, -3.6832e-02, -9.5842e-02,\n",
            "         -1.1229e-01, -6.9068e-02,  9.8392e-02, -4.0490e-02, -2.3849e-03,\n",
            "         -2.8082e-02,  8.3039e-02, -4.9926e-02,  3.1357e-02,  5.2635e-02,\n",
            "         -1.5701e-01, -1.0842e-02, -7.4978e-02,  2.9118e-02, -3.9198e-02,\n",
            "          3.6288e-02, -6.2736e-02,  1.0830e-01,  3.8578e-02, -1.5140e-02,\n",
            "          3.3181e-02, -4.1503e-02,  2.6595e-02,  5.8782e-02,  7.4357e-02,\n",
            "         -7.7146e-02,  8.3363e-02, -4.8879e-03,  8.3363e-02, -3.4078e-03,\n",
            "         -6.4930e-02, -2.3693e-02, -7.8366e-02,  1.5962e-01,  3.6101e-03,\n",
            "         -1.5456e-02, -3.7790e-02, -5.0740e-02,  5.7588e-02, -2.5584e-02,\n",
            "         -9.4863e-03,  1.5484e-01, -1.3355e-02, -1.4859e-02, -8.3055e-02,\n",
            "          1.7021e-02, -2.7734e-02, -6.8668e-02, -5.1734e-02, -1.1687e-01,\n",
            "         -2.0970e-02, -1.1179e-01,  4.2661e-02,  9.2198e-03,  1.3177e-02,\n",
            "          1.7552e-02, -8.0950e-03,  2.5217e-03,  5.1659e-02,  5.3910e-02,\n",
            "         -8.9026e-03, -1.8567e-02,  7.7814e-03,  1.1169e-01,  2.9478e-02,\n",
            "         -3.2939e-02,  2.1833e-02,  2.2672e-02,  1.4534e-01, -4.6424e-02,\n",
            "         -2.5401e-02,  4.3565e-02,  1.8274e-01, -2.7073e-02, -6.6404e-02,\n",
            "          5.5839e-02, -1.0571e-01,  2.9355e-02, -7.5991e-02, -4.1476e-02,\n",
            "          1.1554e-01, -1.0690e-01, -4.1313e-02, -9.8182e-02,  8.0335e-02,\n",
            "         -2.9431e-02,  1.3154e-02,  4.8969e-02,  7.9807e-02, -1.3085e-02,\n",
            "         -4.2453e-02,  1.1871e-01, -6.8764e-02,  9.0751e-02, -1.2017e-02,\n",
            "          5.1942e-02, -1.8024e-02, -1.2120e-01,  5.0255e-02,  1.9909e-02,\n",
            "          2.8550e-02,  4.0770e-02,  9.2552e-02,  9.8602e-02, -7.2846e-03,\n",
            "         -7.5322e-02, -9.1384e-02,  9.1341e-02, -5.7279e-02,  3.4504e-02,\n",
            "          2.4274e-02,  9.5364e-03,  5.6415e-03,  1.3318e-02,  4.1006e-02,\n",
            "         -5.1923e-02,  5.5476e-02, -4.6353e-02,  5.7282e-02, -2.4737e-02,\n",
            "          1.1375e-02, -7.7983e-03,  2.4080e-02, -3.9140e-02, -1.1762e-02,\n",
            "         -6.3238e-02, -6.7358e-02,  1.6056e-02,  4.2361e-02, -4.5173e-02,\n",
            "         -1.9957e-02,  2.9085e-02, -1.2306e-02, -2.1576e-02,  1.6327e-02,\n",
            "         -9.7485e-02,  8.9604e-03,  2.7815e-02,  3.8846e-02, -1.7355e-02,\n",
            "         -2.7158e-02,  8.2030e-02, -1.0045e-01, -4.8950e-02, -1.8653e-02,\n",
            "          9.1789e-02, -4.1941e-02,  3.0935e-03,  9.6708e-03,  8.1613e-02,\n",
            "         -3.5669e-02,  7.8100e-02, -3.2764e-02,  9.5002e-02, -2.5083e-02,\n",
            "          3.5939e-02,  1.1454e-02, -5.3651e-02, -7.8871e-02,  1.4593e-01,\n",
            "          7.3776e-02,  1.1057e-01, -3.6575e-02,  2.3610e-02, -7.3621e-04,\n",
            "          4.3128e-02,  1.6360e-02,  4.1919e-02,  1.4321e-02,  1.6759e-02,\n",
            "          1.0360e-02,  1.3246e-02,  1.9213e-02,  9.7709e-03, -7.2434e-02,\n",
            "         -4.8046e-02, -9.5260e-02,  4.6267e-02,  4.3607e-02,  5.5868e-02,\n",
            "          4.0439e-02,  8.3071e-02, -6.2591e-02,  9.2484e-03,  4.1667e-03,\n",
            "         -6.3098e-02],\n",
            "        [ 1.0854e-02, -1.4883e-02, -1.2944e-02, -8.5836e-02, -1.1338e-01,\n",
            "          7.0753e-02,  3.3035e-02, -2.2413e-02,  5.0306e-02,  6.9992e-02,\n",
            "         -5.3873e-02,  3.8862e-02,  1.3732e-01,  6.5294e-02, -5.4202e-02,\n",
            "          9.4580e-02,  6.4684e-02,  3.8927e-02,  2.2155e-02,  4.4606e-02,\n",
            "         -1.0902e-02,  9.8250e-02, -2.9504e-02,  7.4209e-02, -5.8721e-02,\n",
            "          3.4703e-02,  4.7445e-02,  1.3831e-02,  3.9095e-02,  2.6058e-02,\n",
            "          1.4907e-01,  6.5091e-02, -2.2756e-02, -5.4947e-02,  2.4753e-04,\n",
            "          9.7124e-02, -6.4832e-03,  5.6309e-02,  1.2335e-01, -4.7933e-02,\n",
            "         -7.6302e-02,  8.3756e-02, -3.0621e-02,  5.3587e-02, -5.2614e-02,\n",
            "          1.0096e-01, -9.4968e-02,  2.1975e-02,  5.6247e-02, -1.7746e-02,\n",
            "         -1.1653e-01, -1.5021e-01,  6.8528e-02,  5.6274e-02,  2.1219e-02,\n",
            "          7.8613e-02,  3.7878e-03,  1.0598e-01, -9.9663e-03,  7.6521e-02,\n",
            "          1.0257e-01, -8.4066e-03, -6.4333e-02,  8.0766e-02, -2.4581e-02,\n",
            "         -2.3191e-02, -4.8181e-02,  3.2599e-02, -1.7375e-02, -4.6086e-02,\n",
            "         -5.6351e-02, -4.2994e-02,  7.1839e-02, -5.1103e-02, -1.2019e-03,\n",
            "          8.9204e-03,  1.0633e-01, -7.0575e-02,  3.9993e-02,  5.3107e-02,\n",
            "         -1.1274e-01, -2.0476e-02, -8.1580e-02,  1.7478e-02, -5.8710e-02,\n",
            "          1.0718e-02, -5.0749e-03,  9.7214e-02, -5.4283e-03, -2.3976e-02,\n",
            "          2.7422e-02, -6.4775e-02, -7.1353e-03,  5.5354e-02,  8.2700e-02,\n",
            "         -8.6156e-02,  6.1676e-02, -7.1749e-03,  6.5775e-02,  2.2252e-02,\n",
            "         -2.7773e-02, -1.9527e-02, -5.1926e-02,  1.5596e-01,  7.9583e-03,\n",
            "         -6.2363e-02, -1.0382e-02, -4.4100e-02,  5.3953e-02, -3.8602e-02,\n",
            "         -8.9214e-02,  1.6656e-01,  2.0963e-02, -3.0304e-02, -5.0568e-02,\n",
            "          1.5758e-02,  1.8376e-03, -6.4169e-02, -6.5330e-02, -5.7867e-02,\n",
            "         -9.0029e-02, -7.1962e-02, -1.2056e-02, -2.3751e-02,  1.2404e-02,\n",
            "          2.3616e-03, -2.7421e-03, -1.4166e-02,  2.6630e-02,  9.4547e-02,\n",
            "         -7.5422e-02, -5.4859e-02, -1.7579e-02,  1.3934e-01, -7.9771e-03,\n",
            "         -2.2405e-02,  4.4987e-02,  2.5018e-02,  1.3742e-01, -7.3263e-02,\n",
            "         -8.8214e-03,  2.6553e-02,  1.4798e-01,  2.0231e-02, -9.8148e-02,\n",
            "          4.6147e-02, -1.0703e-01,  2.8402e-03, -3.5522e-02, -4.2626e-02,\n",
            "          1.2549e-01, -1.3641e-01, -3.4221e-02, -1.2490e-01,  9.8571e-02,\n",
            "         -8.5750e-02,  4.3367e-03,  9.0984e-02,  9.1696e-02, -1.8843e-03,\n",
            "         -2.5656e-02,  8.8407e-02, -7.2900e-02,  6.2928e-02,  1.0155e-02,\n",
            "          4.0066e-02,  5.6939e-03, -7.0069e-02,  3.4845e-02,  3.7699e-02,\n",
            "          3.0469e-02, -7.2027e-03,  1.1661e-01,  1.1119e-01, -4.2782e-02,\n",
            "         -5.7612e-02, -8.6255e-02,  7.5178e-02, -1.9575e-02,  2.8452e-02,\n",
            "          8.2286e-02,  3.7815e-02, -2.0217e-02,  7.0294e-02,  5.1269e-02,\n",
            "         -7.9361e-02,  5.9854e-02, -1.4400e-02,  8.5482e-02, -4.4805e-02,\n",
            "          1.8464e-02, -3.2331e-02,  1.2807e-02, -4.1945e-02, -5.3523e-03,\n",
            "         -9.1169e-02, -8.7797e-02, -3.3455e-02,  3.9584e-02, -7.1719e-02,\n",
            "         -3.7091e-02,  3.9037e-02, -1.5002e-02, -3.7078e-02,  2.1392e-02,\n",
            "         -1.4448e-01,  2.7719e-02,  2.4924e-02,  5.0025e-02,  1.0609e-02,\n",
            "          2.0774e-02,  3.5454e-02, -7.7223e-02, -3.9381e-02,  1.1087e-03,\n",
            "          5.8633e-02, -1.7998e-02, -2.3518e-03, -1.9785e-04,  6.4662e-02,\n",
            "         -3.5204e-02,  9.2379e-02, -5.8599e-02,  9.2819e-02, -3.2028e-02,\n",
            "         -1.6459e-02,  8.3611e-02, -8.9147e-03, -8.0083e-02,  1.2163e-01,\n",
            "          7.4460e-02,  9.6498e-02, -7.2343e-02,  1.4065e-02,  2.1513e-04,\n",
            "          1.8317e-02, -7.3986e-03,  1.3138e-02,  4.3968e-02,  5.0258e-03,\n",
            "         -3.9009e-02, -6.2485e-03,  2.8481e-02,  4.6992e-02, -5.6997e-02,\n",
            "         -5.0557e-02, -6.6886e-02,  3.2855e-03,  4.9285e-02,  4.1738e-02,\n",
            "          1.6443e-02,  8.9398e-02, -6.1259e-02,  3.8919e-02, -3.1421e-03,\n",
            "         -2.7516e-02],\n",
            "        [ 2.9710e-02, -4.1256e-02, -2.9152e-02, -1.0334e-01, -8.7319e-02,\n",
            "          1.3010e-01,  3.1076e-02,  6.0060e-02,  6.4801e-02,  3.5320e-02,\n",
            "          1.3057e-02,  2.1602e-02,  8.3175e-02,  1.4218e-01,  5.2153e-03,\n",
            "          1.0671e-01,  9.2915e-02,  6.3959e-03,  7.6569e-03,  5.0293e-02,\n",
            "          4.3667e-02,  5.3194e-02, -4.5878e-02,  6.5333e-02, -6.6637e-02,\n",
            "          4.8820e-02,  7.0628e-02, -1.9331e-02,  1.8388e-02,  9.9676e-03,\n",
            "          1.4517e-01,  4.6954e-02, -1.8143e-02, -2.9118e-02,  1.5375e-03,\n",
            "          8.3592e-02, -7.8094e-03,  4.5004e-04,  1.4260e-01, -4.7374e-02,\n",
            "         -1.9813e-02,  4.1528e-02,  1.6304e-02,  6.7689e-02, -6.7725e-02,\n",
            "          7.1546e-02, -1.3351e-01, -3.5042e-03,  8.3407e-02, -7.7943e-02,\n",
            "         -1.3927e-01, -1.1041e-01,  5.0595e-02,  7.5261e-02,  1.9466e-02,\n",
            "          5.5912e-02,  1.4601e-02,  1.3165e-01, -2.0958e-02,  6.3104e-02,\n",
            "          1.2276e-01, -1.8375e-02, -7.7003e-02,  3.6939e-02, -2.0724e-02,\n",
            "         -5.6370e-03,  2.8220e-03,  1.5044e-02, -5.3447e-02, -4.0793e-02,\n",
            "         -7.0952e-02, -3.7236e-02,  8.1425e-02, -1.7507e-02, -2.9745e-02,\n",
            "         -8.9391e-03,  2.7039e-02, -3.7859e-04,  1.7992e-02,  4.0227e-02,\n",
            "         -1.4305e-01, -6.0473e-02, -4.5385e-02, -1.0683e-02, -2.2007e-02,\n",
            "          7.1250e-02, -6.0989e-02,  6.3779e-02,  9.5614e-02,  4.0855e-03,\n",
            "          8.7183e-02, -6.9716e-02,  3.8823e-02,  1.7551e-02,  5.8610e-02,\n",
            "         -4.9795e-02,  8.0417e-02, -2.5064e-02,  5.4811e-02, -1.7525e-03,\n",
            "         -3.6044e-02,  3.0767e-02, -5.0139e-02,  1.3747e-01, -1.5660e-02,\n",
            "         -3.8895e-03, -2.3145e-02, -3.6336e-02,  7.3963e-02, -6.5408e-02,\n",
            "          1.7788e-02,  1.3527e-01, -5.2830e-02, -9.3090e-03, -9.9534e-02,\n",
            "         -2.8430e-02, -1.5969e-02, -8.3923e-02, -4.7450e-02, -1.2377e-01,\n",
            "         -1.6993e-02, -6.3066e-02,  3.5410e-02,  3.5869e-02,  1.2481e-02,\n",
            "          2.6665e-02, -2.8223e-02,  2.0767e-02,  7.0052e-02,  7.2694e-02,\n",
            "         -1.9689e-02, -6.6217e-02,  2.1277e-02,  1.0835e-01,  5.0354e-02,\n",
            "         -1.3383e-02, -3.6718e-02, -1.3454e-03,  1.3929e-01, -2.4763e-02,\n",
            "         -8.8719e-03,  5.4466e-02,  1.3190e-01,  4.1222e-03, -8.4283e-02,\n",
            "          1.0055e-01, -9.4520e-02,  2.0323e-02, -8.8467e-02, -4.0716e-02,\n",
            "          1.0519e-01, -1.3324e-01, -5.6986e-02, -8.9764e-02,  1.0056e-01,\n",
            "         -4.0248e-02,  2.3813e-02,  5.6333e-02,  6.1664e-02, -1.9672e-02,\n",
            "         -6.4284e-02,  5.0841e-02, -6.9556e-02, -4.7642e-03, -2.6892e-02,\n",
            "          9.3863e-02, -2.3153e-02, -1.2560e-01,  3.4182e-02,  1.2374e-02,\n",
            "          7.4799e-02,  4.9561e-02,  8.6421e-02,  9.7478e-02,  4.5795e-02,\n",
            "         -4.5387e-02, -7.0113e-02,  1.2225e-01, -5.6533e-02,  4.2442e-02,\n",
            "          1.6379e-04, -3.5969e-03,  1.5735e-02,  2.5455e-02,  8.3635e-02,\n",
            "         -3.2964e-02,  5.0570e-02, -5.3760e-02,  9.6966e-02, -3.3584e-02,\n",
            "         -5.7311e-04, -9.4525e-04,  5.5335e-02, -2.4468e-02,  3.1579e-02,\n",
            "         -5.0859e-02, -6.8448e-02,  1.0837e-02,  6.8804e-02, -3.7826e-02,\n",
            "         -1.5227e-03,  1.7398e-02, -6.7580e-02, -8.5191e-02,  1.0101e-01,\n",
            "         -1.0047e-01,  2.6223e-02,  7.7122e-03,  7.3595e-02, -1.1101e-02,\n",
            "         -3.3759e-05,  4.4067e-02, -7.3593e-02, -3.1317e-02, -5.7726e-02,\n",
            "          8.3262e-02, -2.7371e-02, -1.0974e-02,  2.6631e-03,  5.3114e-02,\n",
            "          1.1463e-02,  5.5632e-02, -6.9308e-02,  6.3216e-02,  1.5895e-03,\n",
            "          1.2178e-02,  4.1109e-02,  2.3224e-03, -9.3300e-02,  1.6917e-01,\n",
            "          6.5348e-02,  8.9860e-02, -5.3080e-02,  1.0889e-02,  4.6475e-03,\n",
            "          2.3398e-02,  6.4662e-02,  2.1169e-02,  8.0970e-02, -1.4400e-02,\n",
            "          1.0675e-03,  5.5733e-03, -1.0189e-02,  2.2934e-02, -5.9987e-02,\n",
            "         -2.3941e-02, -1.3221e-01,  1.6447e-02,  5.2509e-02,  2.9657e-02,\n",
            "          5.2244e-02,  8.0499e-02, -6.4033e-02,  1.1377e-02,  1.3095e-02,\n",
            "         -4.4125e-02]], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(tensor([[ 0.0000],\n",
            "        [-0.0423],\n",
            "        [ 0.0223]], device='cuda:0', grad_fn=<SubBackward0>), tensor([[ 0.0000],\n",
            "        [-0.0423],\n",
            "        [ 0.0223]], device='cuda:0', grad_fn=<SubBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_module.dataset[0][1]"
      ],
      "metadata": {
        "id": "3Sa7xaWK2IEb",
        "outputId": "54fecfc7-10be-425e-9414-b3385e28e3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/CV/CVUSA_subset/bingmap/input0000008.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search"
      ],
      "metadata": {
        "id": "-AuLAdBeryMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # def search(self, grd_descriptor, sat_descriptor,topK):\n",
        "\n",
        "  #       dist_array = 2 - 2 * np.matmul(sat_descriptor.cpu(), np.transpose(grd_descriptor.cpu()))\n",
        "  #       topk_indices = []\n",
        "\n",
        "  #       for i in range(dist_array.shape[0]):\n",
        "  #           gt_dist = dist_array[i, i]\n",
        "  #           distances = dist_array[i, :]\n",
        "  #           distances[i] = np.inf  # Escludi la distanza con se stesso\n",
        "\n",
        "  #           # Trova gli indici delle top-K immagini (con distanze minori di gt_dist)\n",
        "  #           topk_indices_i = np.argsort(distances)[:topK]\n",
        "  #           topk_indices.append(topk_indices_i)\n",
        "\n",
        "  #       return topk_indices"
      ],
      "metadata": {
        "id": "vHTQ9xGEtTEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SIFT APPROACH"
      ],
      "metadata": {
        "id": "YHv90khTDZ6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##UNIVERSITY"
      ],
      "metadata": {
        "id": "jOInv_qCa9c4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GROUND"
      ],
      "metadata": {
        "id": "a0CzJwB8bGgs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGDhV8ee7hwv"
      },
      "outputs": [],
      "source": [
        "# Definisci il percorso della cartella contenente le sottocartelle con le immagini\n",
        "source_dir = \"/content/University-Release/train/street\"\n",
        "\n",
        "# Dizionario per contenere le immagini\n",
        "data = {}\n",
        "\n",
        "# Itera su tutte le sottocartelle nella cartella di origine\n",
        "for i, folder_name in enumerate(sorted(os.listdir(source_dir))):\n",
        "    # Crea il percorso completo della sottocartella\n",
        "    folder_path = os.path.join(source_dir, folder_name)\n",
        "    # Se la sottocartella è una cartella\n",
        "    if os.path.isdir(folder_path):\n",
        "        # Lista per contenere le immagini della cartella corrente\n",
        "        folder_images = []\n",
        "        # Itera su tutti i file all'interno della sottocartella\n",
        "        for filename in sorted(os.listdir(folder_path)):\n",
        "            # Crea il percorso completo del file\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            # Carica l'immagine\n",
        "            image = cv2.imread(file_path)\n",
        "            # Aggiungi l'immagine alla lista delle immagini della cartella corrente\n",
        "            folder_images.append(image)\n",
        "        # Aggiungi le immagini della cartella al dizionario data\n",
        "        data[i] = folder_images\n",
        "\n",
        "# Visualizza il risultato\n",
        "for folder_id, images in data.items():\n",
        "    print(f\"Folder ID: {folder_id}, Numero di immagini: {len(images)}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3o9xx1f-PpQ"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(data[0][0])\n",
        "print(data[1][1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfILW83ZCwna"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A2yY1vc-rUJ"
      },
      "outputs": [],
      "source": [
        "images_training = []\n",
        "for images in data.values():\n",
        "  for image in images:\n",
        "    images_training.append(image)\n",
        "print(len(images_training))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cv2_imshow(images_training[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YSXbKX9eIoC"
      },
      "outputs": [],
      "source": [
        "# convert images to grayscale\n",
        "bw_images_g = []\n",
        "for img in tqdm(images_training):\n",
        "    # if RGB, transform into grayscale\n",
        "    if len(img.shape) == 3:\n",
        "        bw_images_g.append(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
        "    else:\n",
        "        # if grayscale, do not transform\n",
        "        bw_images_g.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbFJVP68eLmV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(bw_images[1], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uZwi-YmePGm"
      },
      "outputs": [],
      "source": [
        "# defining feature extractor that we want to use (SIFT)\n",
        "extractor = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "# initialize lists where we will store *all* keypoints and descriptors\n",
        "keypoints = []\n",
        "descriptors = []\n",
        "\n",
        "for img in tqdm(bw_images):\n",
        "    # extract keypoints and descriptors for each image\n",
        "    img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)\n",
        "    keypoints.append(img_keypoints)\n",
        "    descriptors.append(img_descriptors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM4mHTqCePnx"
      },
      "outputs": [],
      "source": [
        "print(f\"len before: {len(descriptors)}\")\n",
        "# initialize list to store idx values of records to drop\n",
        "to_drop = []\n",
        "for i, img_descriptors in enumerate(descriptors):\n",
        "    # if there are no descriptors, add record idx to drop list\n",
        "    if img_descriptors is None:\n",
        "        to_drop.append(i)\n",
        "\n",
        "print(f\"indexes: {to_drop}\")\n",
        "# delete from list in reverse order\n",
        "for i in sorted(to_drop, reverse=True):\n",
        "    del descriptors[i], keypoints[i]\n",
        "\n",
        "print(f\"len after: {len(descriptors)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p03AcfPAeS5d"
      },
      "outputs": [],
      "source": [
        "output_image = []\n",
        "for x in range(3):\n",
        "    output_image.append(cv2.drawKeypoints(bw_images[x], keypoints[x], 0, (255, 0, 0),\n",
        "                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
        "    plt.imshow(output_image[x], cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDS2MIA2eXam"
      },
      "outputs": [],
      "source": [
        "# select the same numbers in each run\n",
        "np.random.seed(0)\n",
        "# select 1000 random image index values\n",
        "sample_idx = np.random.randint(0, len(data)+1, 1000).tolist()\n",
        "len(sample_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxcNGG9_eZcl"
      },
      "outputs": [],
      "source": [
        "# extract the sample from descriptors\n",
        "# (we don't need keypoints)\n",
        "descriptors_sample = []\n",
        "\n",
        "for n in tqdm(sample_idx):\n",
        "    descriptors_sample.append(np.array(descriptors[n]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYb69Ff9ecjF"
      },
      "outputs": [],
      "source": [
        "all_descriptors = []\n",
        "# extract image descriptor lists\n",
        "for img_descriptors in tqdm(descriptors_sample):\n",
        "    # extract specific descriptors within the image\n",
        "    for descriptor in img_descriptors:\n",
        "        all_descriptors.append(descriptor)\n",
        "# convert to single numpy array\n",
        "all_descriptors = np.stack(all_descriptors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMCbY1RKeeXp"
      },
      "outputs": [],
      "source": [
        "# check the shape\n",
        "all_descriptors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx-aN_elegaG"
      },
      "outputs": [],
      "source": [
        "# we can count the number of descriptors contained in descriptors to confirm\n",
        "count = []\n",
        "for img_descriptors in descriptors_sample:\n",
        "    count.append(len(img_descriptors))\n",
        "# here we can see the number of descriptors for the first five images\n",
        "print(f\"first five: {count[:5]}\")\n",
        "# and if we sum them all, we should see the 39893 from before\n",
        "print(f\"count all: {sum(count)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZpbDuvceitP"
      },
      "outputs": [],
      "source": [
        "# perform k-means clustering to build the codebook\n",
        "from scipy.cluster.vq import kmeans\n",
        "\n",
        "k = 200\n",
        "iters = 1\n",
        "codebook, variance = kmeans(all_descriptors, k, iters) # It takes approx. 2-3 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bhr8ILSTenC8"
      },
      "outputs": [],
      "source": [
        "# vector quantization\n",
        "from scipy.cluster.vq import vq\n",
        "\n",
        "visual_words = []\n",
        "for img_descriptors in tqdm(descriptors):\n",
        "    # for each image, map each descriptor to the nearest codebook entry\n",
        "    img_visual_words, distance = vq(img_descriptors, codebook)\n",
        "    visual_words.append(img_visual_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgRR1sxseo3e"
      },
      "outputs": [],
      "source": [
        "# let's see what the visual words look like for image 0\n",
        "visual_words[0][:5], len(visual_words[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhauGjf7etd2"
      },
      "outputs": [],
      "source": [
        "frequency_vectors = []\n",
        "for img_visual_words in tqdm(visual_words):\n",
        "    # create a frequency vector for each image\n",
        "    img_frequency_vector = np.zeros(k)\n",
        "    for word in img_visual_words:\n",
        "        img_frequency_vector[word] += 1\n",
        "    frequency_vectors.append(img_frequency_vector)\n",
        "# stack together in numpy array\n",
        "frequency_vectors = np.stack(frequency_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt_K9NsuevBj"
      },
      "outputs": [],
      "source": [
        "frequency_vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh3XnjL8ew9l"
      },
      "outputs": [],
      "source": [
        "# we know from above that ids 84, 22, 45, and 172 appear in image 0\n",
        "for i in [84,  22,  45, 172]:\n",
        "    print(f\"{i}: {frequency_vectors[0][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1noFCx-Eeyt5"
      },
      "outputs": [],
      "source": [
        "frequency_vectors[0][:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjiYnAvde0LO"
      },
      "outputs": [],
      "source": [
        "plt.bar(list(range(k)), frequency_vectors[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17IxXbL8e0rO"
      },
      "outputs": [],
      "source": [
        "# N is the number of images, i.e. the size of the dataset\n",
        "N = 2659\n",
        "\n",
        "# df is the number of images that a visual word appears in\n",
        "# we calculate it by counting non-zero values as 1 and summing\n",
        "df = np.sum(frequency_vectors > 0, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SaS2YL8fEcA"
      },
      "outputs": [],
      "source": [
        "df.shape, df[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izWvkZ3qfF_g"
      },
      "outputs": [],
      "source": [
        "idf = np.log(N/ df)\n",
        "idf.shape, idf[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Icnwu07fIlk"
      },
      "outputs": [],
      "source": [
        "tfidf = frequency_vectors * idf\n",
        "tfidf.shape, tfidf[0][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YECNCnmKfKYh"
      },
      "outputs": [],
      "source": [
        "plt.bar(list(range(k)), tfidf[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKIJ_4P3fK3Z"
      },
      "outputs": [],
      "source": [
        "search_i = 0\n",
        "\n",
        "plt.imshow(bw_images[search_i], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwtP64VDfPmG"
      },
      "outputs": [],
      "source": [
        "# cosine similarity\n",
        "from numpy.linalg import norm\n",
        "\n",
        "a = tfidf[search_i]\n",
        "b = tfidf  # set search space to the full sample\n",
        "\n",
        "cosine_similarity = np.dot(a, b.T)/(norm(a) * norm(b, axis=1))\n",
        "print(\"Min cosine similarity:\", round(np.min(cosine_similarity),1))\n",
        "print(\"Max cosine similarity:\", np.max(cosine_similarity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x70icggfRMR"
      },
      "outputs": [],
      "source": [
        "cosine_similarity.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_unEIJ5fSmc"
      },
      "outputs": [],
      "source": [
        "cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qws74Q1ffUUc"
      },
      "outputs": [],
      "source": [
        "top_k = 5\n",
        "idx = np.argsort(-cosine_similarity)[:top_k]\n",
        "idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93AjBD6SfV4d"
      },
      "outputs": [],
      "source": [
        "cosine_similarity[idx[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUraJ5dWfYAu"
      },
      "outputs": [],
      "source": [
        "for i in idx:\n",
        "    print(f\"{i}: {round(cosine_similarity[i], 4)}\")\n",
        "    plt.imshow(bw_images[i], cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRcw9NzhfZ8Y"
      },
      "outputs": [],
      "source": [
        "def search(i: int, top_k: int = 5):\n",
        "    print(\"Search image:\")\n",
        "    # show the search image\n",
        "    plt.imshow(bw_images[i], cmap='gray')\n",
        "    plt.show()\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    # get search image vector\n",
        "    a = tfidf[i]\n",
        "    # get the cosine distance for the search image `a`\n",
        "    cosine_similarity = np.dot(a, b.T)/(norm(a) * norm(b, axis=1))\n",
        "    # get the top k indices for most similar vecs\n",
        "    idx = np.argsort(-cosine_similarity)[:top_k]\n",
        "    # display the results\n",
        "    for i in idx:\n",
        "        print(f\"{i}: {round(cosine_similarity[i], 4)}\")\n",
        "        plt.imshow(bw_images[i], cmap='gray')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77qNrLrufbk9"
      },
      "outputs": [],
      "source": [
        "search(121)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uaanl3zTfc8h"
      },
      "outputs": [],
      "source": [
        "search(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3,7):\n",
        "  cv2_imshow(images_training[i])"
      ],
      "metadata": {
        "id": "0_JVS0yHmczh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0KN0Rj_feFo"
      },
      "outputs": [],
      "source": [
        "search(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W_LVSIQpRBJ"
      },
      "source": [
        "### AERIAL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisci il percorso della cartella contenente le sottocartelle con le immagini\n",
        "source_dir = \"/content/University-Release/train/satellite\"\n",
        "\n",
        "# Dizionario per contenere le immagini\n",
        "data = {}\n",
        "\n",
        "# Itera su tutte le sottocartelle nella cartella di origine\n",
        "for i, folder_name in enumerate(sorted(os.listdir(source_dir))):\n",
        "    # Crea il percorso completo della sottocartella\n",
        "    folder_path = os.path.join(source_dir, folder_name)\n",
        "    # Se la sottocartella è una cartella\n",
        "    if os.path.isdir(folder_path):\n",
        "        # Lista per contenere le immagini della cartella corrente\n",
        "        folder_images = []\n",
        "        # Itera su tutti i file all'interno della sottocartella\n",
        "        for filename in sorted(os.listdir(folder_path)):\n",
        "            # Crea il percorso completo del file\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            # Carica l'immagine\n",
        "            image = cv2.imread(file_path)\n",
        "            # Aggiungi l'immagine alla lista delle immagini della cartella corrente\n",
        "            folder_images.append(image)\n",
        "        # Aggiungi le immagini della cartella al dizionario data\n",
        "        data[i] = folder_images\n",
        "\n",
        "# Visualizza il risultato\n",
        "for folder_id, images in data.items():\n",
        "    print(f\"Folder ID: {folder_id}, Numero di immagini: {len(images)}\")"
      ],
      "metadata": {
        "id": "ChmjVGkSpsoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZACc_U2npRBP"
      },
      "outputs": [],
      "source": [
        "images_training = []\n",
        "for images in data.values():\n",
        "  for image in images:\n",
        "    images_training.append(image)\n",
        "print(len(images_training))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cv2_imshow(images_training[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhou3n4PpRBQ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# convert images to grayscale\n",
        "bw_images = []\n",
        "for img in tqdm(images_training):\n",
        "    # if RGB, transform into grayscale\n",
        "    if len(img.shape) == 3:\n",
        "        bw_images.append(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
        "    else:\n",
        "        # if grayscale, do not transform\n",
        "        bw_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwnlJziqpRBQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(bw_images[1], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T32FHbv-pRBQ"
      },
      "outputs": [],
      "source": [
        "# defining feature extractor that we want to use (SIFT)\n",
        "extractor = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "# initialize lists where we will store *all* keypoints and descriptors\n",
        "keypoints = []\n",
        "descriptors = []\n",
        "\n",
        "for img in tqdm(bw_images):\n",
        "    # extract keypoints and descriptors for each image\n",
        "    img_keypoints, img_descriptors = extractor.detectAndCompute(img, None)\n",
        "    keypoints.append(img_keypoints)\n",
        "    descriptors.append(img_descriptors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppRySWZlpRBQ"
      },
      "outputs": [],
      "source": [
        "print(f\"len before: {len(descriptors)}\")\n",
        "# initialize list to store idx values of records to drop\n",
        "to_drop = []\n",
        "for i, img_descriptors in enumerate(descriptors):\n",
        "    # if there are no descriptors, add record idx to drop list\n",
        "    if img_descriptors is None:\n",
        "        to_drop.append(i)\n",
        "\n",
        "print(f\"indexes: {to_drop}\")\n",
        "# delete from list in reverse order\n",
        "for i in sorted(to_drop, reverse=True):\n",
        "    del descriptors[i], keypoints[i]\n",
        "\n",
        "print(f\"len after: {len(descriptors)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qONT1nU_pRBS"
      },
      "outputs": [],
      "source": [
        "output_image = []\n",
        "for x in range(3):\n",
        "    output_image.append(cv2.drawKeypoints(bw_images[x], keypoints[x], 0, (255, 0, 0),\n",
        "                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
        "    plt.imshow(output_image[x], cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnQctkAopRBS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# select the same numbers in each run\n",
        "np.random.seed(0)\n",
        "# select 1000 random image index values\n",
        "sample_idx = np.random.randint(0, len(data)+1, 50).tolist()\n",
        "len(sample_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwCIAa26pRBS"
      },
      "outputs": [],
      "source": [
        "# extract the sample from descriptors\n",
        "# (we don't need keypoints)\n",
        "descriptors_sample = []\n",
        "\n",
        "for n in tqdm(sample_idx):\n",
        "    descriptors_sample.append(np.array(descriptors[n]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w14ilLQpRBT"
      },
      "outputs": [],
      "source": [
        "all_descriptors = []\n",
        "# extract image descriptor lists\n",
        "for img_descriptors in tqdm(descriptors_sample):\n",
        "    # extract specific descriptors within the image\n",
        "    for descriptor in img_descriptors:\n",
        "        all_descriptors.append(descriptor)\n",
        "# convert to single numpy array\n",
        "all_descriptors = np.stack(all_descriptors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA_lOoMapRBT"
      },
      "outputs": [],
      "source": [
        "# check the shape\n",
        "all_descriptors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgrkFJhapRBT"
      },
      "outputs": [],
      "source": [
        "# we can count the number of descriptors contained in descriptors to confirm\n",
        "count = []\n",
        "for img_descriptors in descriptors_sample:\n",
        "    count.append(len(img_descriptors))\n",
        "# here we can see the number of descriptors for the first five images\n",
        "print(f\"first five: {count[:5]}\")\n",
        "# and if we sum them all, we should see the 39893 from before\n",
        "print(f\"count all: {sum(count)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzWnVaCmpRBT"
      },
      "outputs": [],
      "source": [
        "# perform k-means clustering to build the codebook\n",
        "from scipy.cluster.vq import kmeans\n",
        "\n",
        "k = 200\n",
        "iters = 1\n",
        "codebook, variance = kmeans(all_descriptors, k, iters) # It takes approx. 2-3 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvytgkB0pRBT"
      },
      "outputs": [],
      "source": [
        "# vector quantization\n",
        "from scipy.cluster.vq import vq\n",
        "\n",
        "visual_words = []\n",
        "for img_descriptors in tqdm(descriptors):\n",
        "    # for each image, map each descriptor to the nearest codebook entry\n",
        "    img_visual_words, distance = vq(img_descriptors, codebook)\n",
        "    visual_words.append(img_visual_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxZKcnSupRBT"
      },
      "outputs": [],
      "source": [
        "# let's see what the visual words look like for image 0\n",
        "visual_words[0][:5], len(visual_words[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAxwkfcFpRBU"
      },
      "outputs": [],
      "source": [
        "frequency_vectors = []\n",
        "for img_visual_words in tqdm(visual_words):\n",
        "    # create a frequency vector for each image\n",
        "    img_frequency_vector = np.zeros(k)\n",
        "    for word in img_visual_words:\n",
        "        img_frequency_vector[word] += 1\n",
        "    frequency_vectors.append(img_frequency_vector)\n",
        "# stack together in numpy array\n",
        "frequency_vectors = np.stack(frequency_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0mp51dcpRBU"
      },
      "outputs": [],
      "source": [
        "frequency_vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6mpVsGYpRBU"
      },
      "outputs": [],
      "source": [
        "# we know from above that ids 84, 22, 45, and 172 appear in image 0\n",
        "for i in [84,  22,  45, 172]:\n",
        "    print(f\"{i}: {frequency_vectors[0][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3PMvIdupRBU"
      },
      "outputs": [],
      "source": [
        "frequency_vectors[0][:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg53GOmhpRBU"
      },
      "outputs": [],
      "source": [
        "plt.bar(list(range(k)), frequency_vectors[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fC4yhdKpRBU"
      },
      "outputs": [],
      "source": [
        "# N is the number of images, i.e. the size of the dataset\n",
        "N = 2659\n",
        "\n",
        "# df is the number of images that a visual word appears in\n",
        "# we calculate it by counting non-zero values as 1 and summing\n",
        "df = np.sum(frequency_vectors > 0, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahYJNkYhpRBU"
      },
      "outputs": [],
      "source": [
        "df.shape, df[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw_uVttwpRBU"
      },
      "outputs": [],
      "source": [
        "idf = np.log(N/ df)\n",
        "idf.shape, idf[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NNW2HwQpRBV"
      },
      "outputs": [],
      "source": [
        "tfidf_sat = frequency_vectors * idf\n",
        "tfidf_sat.shape, tfidf[0][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aiao1wenpRBV"
      },
      "outputs": [],
      "source": [
        "plt.bar(list(range(k)), tfidf[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd-PLA43pRBV"
      },
      "outputs": [],
      "source": [
        "search_i = 0\n",
        "\n",
        "plt.imshow(bw_images[search_i], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD-yLrV1pRBV"
      },
      "outputs": [],
      "source": [
        "# cosine similarity\n",
        "from numpy.linalg import norm\n",
        "\n",
        "a_sat = tfidf_sat[search_i]\n",
        "b_sat = tfidf_sat  # set search space to the full sample\n",
        "\n",
        "cosine_similarity = np.dot(a_sat, b_sat.T)/(norm(a_sat) * norm(b_sat, axis=1))\n",
        "print(\"Min cosine similarity:\", round(np.min(cosine_similarity),1))\n",
        "print(\"Max cosine similarity:\", np.max(cosine_similarity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7is6nQkmpRBV"
      },
      "outputs": [],
      "source": [
        "cosine_similarity.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KN3LZ1WpRBV"
      },
      "outputs": [],
      "source": [
        "cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wICs3n62pRBV"
      },
      "outputs": [],
      "source": [
        "top_k = 5\n",
        "idx = np.argsort(-cosine_similarity)[:top_k]\n",
        "idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEqtRQxipRBW"
      },
      "outputs": [],
      "source": [
        "cosine_similarity[idx[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH9OLMy6pRBW"
      },
      "outputs": [],
      "source": [
        "for i in idx:\n",
        "    print(f\"{i}: {round(cosine_similarity[i], 4)}\")\n",
        "    plt.imshow(bw_images[i], cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZOrNP_IpRBW"
      },
      "outputs": [],
      "source": [
        "def search(i: int, top_k: int = 5):\n",
        "    print(\"Search image:\")\n",
        "    # show the search image\n",
        "    plt.imshow(bw_images_g[i], cmap='gray')\n",
        "    plt.show()\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    # get search image vector\n",
        "    a = tfidf[i]\n",
        "    # get the cosine distance for the search image `a`\n",
        "    cosine_similarity = np.dot(a, b_sat.T)/(norm(a) * norm(b_sat, axis=1))\n",
        "    # get the top k indices for most similar vecs\n",
        "    idx = np.argsort(-cosine_similarity)[:top_k]\n",
        "    # display the results\n",
        "    for i in idx:\n",
        "        print(f\"{i}: {round(cosine_similarity[i], 4)}\")\n",
        "        plt.imshow(bw_images[i], cmap='gray')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prELivjipRBW"
      },
      "outputs": [],
      "source": [
        "search(121)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrjMRQElpRBW"
      },
      "outputs": [],
      "source": [
        "search(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3,7):\n",
        "  cv2_imshow(images_training[i])"
      ],
      "metadata": {
        "id": "zB2BgqNGpRBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8NL_qDYpRBW"
      },
      "outputs": [],
      "source": [
        "search(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CVUSA"
      ],
      "metadata": {
        "id": "zhmuKdXDbNvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "uheNeH3hbVV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "GF-22ZhEkGWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creo vettore immagini"
      ],
      "metadata": {
        "id": "4UWuNGCtnjyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Definisci il percorso delle cartelle delle immagini\n",
        "aerial_folder = \"/content/CVUSA_subset/polarmap/normal\"\n",
        "ground_folder = \"/content/CVUSA_subset/streetview\"\n",
        "categories=[]\n",
        "# Carica le immagini ground\n",
        "ground_images = []\n",
        "for filename in sorted(os.listdir(ground_folder)):\n",
        "        image_path = os.path.join(ground_folder, filename)\n",
        "        ground_images.append(cv2.imread(image_path, cv2.IMREAD_GRAYSCALE))\n",
        "        categories.append(\"ground\")\n",
        "# Carica le immagini aerial\n",
        "aerial_images = []\n",
        "for filename in sorted(os.listdir(aerial_folder)):\n",
        "        image_path = os.path.join(aerial_folder, filename)\n",
        "        aerial_images.append(cv2.imread(image_path, cv2.IMREAD_GRAYSCALE))\n",
        "        categories.append(\"aerial\")"
      ],
      "metadata": {
        "id": "ltAD5QKkniCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(ground_images[0])"
      ],
      "metadata": {
        "id": "G8yPvjq1nvIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(aerial_images[0])"
      ],
      "metadata": {
        "id": "Z-T8Lgk7NL5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ground_images[0].shape)\n",
        "print(aerial_images[0].shape)"
      ],
      "metadata": {
        "id": "dsmBdpag2JFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIFT"
      ],
      "metadata": {
        "id": "JlPrc2vTnlx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# Inizializza l'oggetto SIFT\n",
        "sift = cv2.SIFT_create()\n",
        "\n",
        "# Estrai i descrittori SIFT per tutte le immagini ground e aerial\n",
        "\n",
        "ground_descriptors = []\n",
        "kp_ground = []\n",
        "i=0\n",
        "for img in tqdm(ground_images, desc='Processing ground images'):\n",
        "    if(i<1000):\n",
        "      kp, des = sift.detectAndCompute(img, None)\n",
        "      ground_descriptors.append(des)\n",
        "      kp_ground.append(kp)\n",
        "      del kp,des\n",
        "    else: break\n",
        "    i+=1\n",
        "\n"
      ],
      "metadata": {
        "id": "HX9XTVWVkqxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aerial_descriptors = []\n",
        "kp_aerial = []\n",
        "i=0\n",
        "for img in tqdm(aerial_images, desc='Processing aerial images'):\n",
        "    if(i<1000):\n",
        "      kp, des = sift.detectAndCompute(img, None)\n",
        "      aerial_descriptors.append(des)\n",
        "      kp_aerial.append(kp)\n",
        "      del kp,des\n",
        "    else: break\n",
        "    i+=1\n"
      ],
      "metadata": {
        "id": "QOxPs-t9M9rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"len before: {len(ground_descriptors)}\")\n",
        "# initialize list to store idx values of records to drop\n",
        "to_drop = []\n",
        "for i, img_descriptors in enumerate(ground_descriptors):\n",
        "    # if there are no descriptors, add record idx to drop list\n",
        "    if img_descriptors is None:\n",
        "        to_drop.append(i)\n",
        "\n",
        "print(f\"indexes: {to_drop}\")\n",
        "# delete from list in reverse order\n",
        "for i in sorted(to_drop, reverse=True):\n",
        "    del ground_descriptors[i], kp_ground[i]\n",
        "\n",
        "print(f\"len after: {len(ground_descriptors)}\")"
      ],
      "metadata": {
        "id": "4bvo70sfdirl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"len before: {len(aerial_descriptors)}\")\n",
        "# initialize list to store idx values of records to drop\n",
        "to_drop = []\n",
        "for i, img_descriptors in enumerate(aerial_descriptors):\n",
        "    # if there are no descriptors, add record idx to drop list\n",
        "    if img_descriptors is None:\n",
        "        to_drop.append(i)\n",
        "\n",
        "print(f\"indexes: {to_drop}\")\n",
        "# delete from list in reverse order\n",
        "for i in sorted(to_drop, reverse=True):\n",
        "    del aerial_descriptors[i], kp_aerial[i]\n",
        "\n",
        "print(f\"len after: {len(aerial_descriptors)}\")"
      ],
      "metadata": {
        "id": "wuPGkNMfdvfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_descriptors = []\n",
        "descriptors= []\n",
        "# extract image descriptor lists\n",
        "for img_descriptors in tqdm(ground_descriptors):\n",
        "    # extract specific descriptors within the image\n",
        "    for descriptor in img_descriptors:\n",
        "        all_descriptors.append(descriptor)\n",
        "\n",
        "for img_descriptors in tqdm(aerial_descriptors):\n",
        "    # extract specific descriptors within the image\n",
        "    for descriptor in img_descriptors:\n",
        "        all_descriptors.append(descriptor)\n",
        "\n",
        "\n",
        "# extract image descriptor lists\n",
        "for img_descriptors in tqdm(ground_descriptors):\n",
        "    # extract specific descriptors within the image\n",
        "        descriptors.append(img_descriptors)\n",
        "\n",
        "for img_descriptors in tqdm(aerial_descriptors):\n",
        "    # extract specific descriptors within the image\n",
        "        descriptors.append(img_descriptors)\n",
        "\n",
        "# convert to single numpy array\n",
        "all_descriptors = np.stack(all_descriptors)\n",
        "print(all_descriptors.shape)"
      ],
      "metadata": {
        "id": "OWcmy8C-M_mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "output_image = []\n",
        "for x in range(3):\n",
        "    output_image.append(cv2.drawKeypoints(aerial_images[x], kp_aerial[x], 0, (255, 0, 0),\n",
        "                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
        "    plt.imshow(output_image[x], cmap='gray')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2lw5b9x_o0xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from scipy.cluster.vq import kmeans\n",
        "from numpy.linalg import norm\n",
        "num_clusters = 2\n",
        "codebook, variance = kmeans(all_descriptors, 200, 1)\n",
        "# vector quantization\n",
        "from scipy.cluster.vq import vq\n",
        "print(\"shape: \", all_descriptors.shape)\n",
        "print(\"shape: \", codebook.shape)\n",
        "visual_words = []\n",
        "for img_descriptors in tqdm(descriptors):\n",
        "    # for each image, map each descriptor to the nearest codebook entry\n",
        "    img_visual_words, distance = vq(img_descriptors, codebook)\n",
        "    visual_words.append(img_visual_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "04Z86M6Fo0AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_vectors = []\n",
        "for img_visual_words in tqdm(visual_words):\n",
        "    # create a frequency vector for each image\n",
        "    img_frequency_vector = np.zeros(200)\n",
        "    for word in img_visual_words:\n",
        "        img_frequency_vector[word] += 1\n",
        "    frequency_vectors.append(img_frequency_vector)\n",
        "# stack together in numpy array\n",
        "frequency_vectors = np.stack(frequency_vectors)\n",
        "\n",
        "# N is the number of images, i.e. the size of the dataset\n",
        "N = 1998\n",
        "\n",
        "# df is the number of images that a visual word appears in\n",
        "# we calculate it by counting non-zero values as 1 and summing\n",
        "df = np.sum(frequency_vectors > 0, axis=0)\n",
        "idf = np.log(N/ df)\n",
        "tfidf = frequency_vectors * idf"
      ],
      "metadata": {
        "id": "kjsm5Z5y4Sy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(i: int, category: str, top_k: int = 15):\n",
        "    print(\"Search image:\")\n",
        "    # show the search image\n",
        "    if category == 'ground':\n",
        "      plt.imshow(ground_images[i], cmap='gray')\n",
        "      plt.show()\n",
        "      a = tfidf[i]\n",
        "    else:\n",
        "      plt.imshow(aerial_images[i], cmap='gray')\n",
        "      plt.show()\n",
        "      a = tfidf[i+1000]\n",
        "    print(\"-----------------------------------------------------\")\n",
        "    print(tfidf.shape)\n",
        "\n",
        "\n",
        "    # Calculate cosine similarity with images of the opposite category\n",
        "    cosine_similarity = np.dot(a, tfidf.T) / (norm(a) * norm(tfidf, axis=1))\n",
        "\n",
        "    # Get the top k indices for most similar vectors\n",
        "    idx = np.argsort(-cosine_similarity)\n",
        "    idx_filtered=None\n",
        "    if category == 'ground':\n",
        "      idx_filtered = idx[idx >= 1000][:top_k]\n",
        "    else:\n",
        "      idx_filtered = idx[idx < 1000][:top_k]\n",
        "    print(idx_filtered)\n",
        "    # Display the results\n",
        "    for idx_i in idx_filtered:\n",
        "        print(f\"Similarity with image {idx_i} of category {categories[idx_i]}: {round(cosine_similarity[idx_i], 4)}\")\n",
        "        if category == 'ground':\n",
        "          plt.imshow(aerial_images[idx_i-1000], cmap='gray')\n",
        "          plt.show()\n",
        "        else:\n",
        "          plt.imshow(ground_images[idx_i], cmap='gray')\n",
        "          plt.show()\n"
      ],
      "metadata": {
        "id": "ueGvQmUgofRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "  print(\"id: \",i, np.dot(tfidf[i], tfidf[i+1000].T) / (norm(tfidf[i]) * norm(tfidf[i+1000])))"
      ],
      "metadata": {
        "id": "lIRob4Neigon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search(9,'ground')"
      ],
      "metadata": {
        "id": "7GNbDv_e5vEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ImageSimilarityCalculator:\n",
        "    def __init__(self, aerial_link, ground_link):\n",
        "        self.aerial_link = aerial_link\n",
        "        self.ground_link = ground_link\n",
        "        self.num_clusters = 200\n",
        "        self.sift = cv2.SIFT_create()\n",
        "\n",
        "    def load_images(self, folder):\n",
        "        images = []\n",
        "        for filename in sorted(os.listdir(folder)):\n",
        "            image_path = os.path.join(folder, filename)\n",
        "            images.append(cv2.imread(image_path, cv2.IMREAD_GRAYSCALE))\n",
        "        return images\n",
        "\n",
        "    def process_images(self, images):\n",
        "        descriptors = []\n",
        "        keypoints = []\n",
        "        for img in tqdm(images, desc='Processing images'):\n",
        "            kp, des = self.sift.detectAndCompute(img, None)\n",
        "            descriptors.append(des)\n",
        "            keypoints.append(kp)\n",
        "        return descriptors, keypoints\n",
        "\n",
        "    def remove_invalid_descriptors(self, descriptors, keypoints):\n",
        "        to_drop = []\n",
        "        for i, img_descriptors in enumerate(descriptors):\n",
        "            if img_descriptors is None:\n",
        "                to_drop.append(i)\n",
        "        for i in sorted(to_drop, reverse=True):\n",
        "            del descriptors[i], keypoints[i]\n",
        "        return descriptors, keypoints\n",
        "\n",
        "    def concatenate_descriptors(self, descriptors):\n",
        "        all_descriptors = np.concatenate(descriptors, axis=0)\n",
        "        return all_descriptors\n",
        "\n",
        "    def visualize_keypoints(self, images, keypoints):\n",
        "        output_images = []\n",
        "        for img, kp in zip(images, keypoints):\n",
        "            output_images.append(cv2.drawKeypoints(img, kp, 0, (255, 0, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
        "            plt.imshow(output_images[-1], cmap='gray')\n",
        "            plt.show()\n",
        "\n",
        "    def calculate_similarity(self, tfidf_descriptors):\n",
        "        num_ground_images = len(self.ground_images)\n",
        "        num_aerial_images = len(self.aerial_images)\n",
        "        for i in range(num_ground_images):\n",
        "            for j in range(num_aerial_images):\n",
        "                similarity = cosine_similarity(tfidf_descriptors[i], tfidf_descriptors[j + num_ground_images])\n",
        "                print(f\"Similarity between ground image {i} and aerial image {j}:\", similarity)\n",
        "\n",
        "    def process(self):\n",
        "        # Load images\n",
        "        self.ground_images = self.load_images(self.ground_link)\n",
        "        self.aerial_images = self.load_images(self.aerial_link)\n",
        "\n",
        "        # Process images\n",
        "        self.ground_descriptors, self.kp_ground = self.process_images(self.ground_images)\n",
        "        self.aerial_descriptors, self.kp_aerial = self.process_images(self.aerial_images)\n",
        "\n",
        "        # Remove invalid descriptors\n",
        "        self.ground_descriptors, self.kp_ground = self.remove_invalid_descriptors(self.ground_descriptors, self.kp_ground)\n",
        "        self.aerial_descriptors, self.kp_aerial = self.remove_invalid_descriptors(self.aerial_descriptors, self.kp_aerial)\n",
        "\n",
        "        # Concatenate descriptors\n",
        "        all_ground_descriptors = self.concatenate_descriptors(self.ground_descriptors)\n",
        "        all_aerial_descriptors = self.concatenate_descriptors(self.aerial_descriptors)\n",
        "        all_descriptors = np.concatenate((all_ground_descriptors, all_aerial_descriptors), axis=0)\n",
        "\n",
        "        # Visualize keypoints\n",
        "        self.visualize_keypoints(self.aerial_images[:3], self.kp_aerial[:3])\n",
        "\n",
        "        # Utilize K-Means for clustering\n",
        "        kmeans = KMeans(n_clusters=self.num_clusters)\n",
        "        kmeans.fit(all_descriptors)\n",
        "        codebook = kmeans.cluster_centers_\n",
        "\n",
        "        # Calculate cluster labels\n",
        "        labels = kmeans.predict(all_descriptors)\n",
        "\n",
        "        # Calculate histograms\n",
        "        histograms = []\n",
        "        start_index = 0\n",
        "        for descriptors, images in [(self.ground_descriptors, self.ground_images), (self.aerial_descriptors, self.aerial_images)]:\n",
        "            for img in images:\n",
        "                end_index = start_index + len(self.sift.detectAndCompute(img, None)[1])\n",
        "                histogram = np.bincount(labels[start_index:end_index], minlength=self.num_clusters)\n",
        "                histograms.append(histogram)\n",
        "                start_index = end_index\n",
        "\n",
        "        # Calculate TF-IDF\n",
        "        tfidf_transformer = TfidfTransformer()\n",
        "        tfidf_descriptors = tfidf_transformer.fit_transform(histograms)\n",
        "\n",
        "        # Calculate similarity\n",
        "        self.calculate_similarity(tfidf_descriptors)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dEcdoVfyg2b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aerial_link = \"/content/CVUSA_subset/polarmap/normal\"\n",
        "ground_link = \"/content/CVUSA_subset/streetview\"\n",
        "similarity_calculator = ImageSimilarityCalculator(aerial_link, ground_link)\n",
        "similarity_calculator.process()"
      ],
      "metadata": {
        "id": "VbgvzTXpjO-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Siamese Approach"
      ],
      "metadata": {
        "id": "Klmeyr0iD0D2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the lists"
      ],
      "metadata": {
        "id": "RdbHtBfKRxDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Definisci il percorso delle cartelle delle immagini\n",
        "aerial_folder = \"/content/CVUSA_subset/polarmap/normal\"\n",
        "ground_folder = \"/content/CVUSA_subset/streetview\"\n",
        "categories=[]\n",
        "\n",
        "# Carica le immagini ground\n",
        "ground_images = []\n",
        "for filename in sorted(os.listdir(ground_folder)):\n",
        "    image_path = os.path.join(ground_folder, filename)\n",
        "    img = cv2.imread(image_path)\n",
        "    resized_img = cv2.resize(img, (256, 256))  # Ridimensiona l'immagine a 64x64\n",
        "    ground_images.append(resized_img)\n",
        "    categories.append(\"ground\")\n",
        "\n",
        "# Carica le immagini aerial\n",
        "aerial_images = []\n",
        "for filename in sorted(os.listdir(aerial_folder)):\n",
        "    image_path = os.path.join(aerial_folder, filename)\n",
        "    img = cv2.imread(image_path)\n",
        "    resized_img = cv2.resize(img, (256, 256))  # Ridimensiona l'immagine a 64x64\n",
        "    aerial_images.append(resized_img)\n",
        "    categories.append(\"aerial\")\n"
      ],
      "metadata": {
        "id": "ECgPqFVoRwnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aerial_images[0]"
      ],
      "metadata": {
        "id": "0uhksUJGhS1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network"
      ],
      "metadata": {
        "id": "0axGT2PKHxPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import random_split\n",
        "import logging\n",
        "from collections import OrderedDict\n",
        "\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning import Trainer\n",
        "from torch import optim\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from pytorch_lightning.callbacks.progress import RichProgressBar\n",
        "\n",
        "# Define dataset class\n",
        "class ImagePairDataset(Dataset):\n",
        "    def __init__(self, ground_images, aerial_images, transform=None):\n",
        "        self.ground_images = ground_images\n",
        "        self.aerial_images = aerial_images\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ground_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ground_image = self.ground_images[idx]\n",
        "        aerial_image = self.aerial_images[idx]\n",
        "        if self.transform:\n",
        "            ground_image = self.transform(ground_image)\n",
        "            aerial_image = self.transform(aerial_image)\n",
        "        return ground_image, aerial_image\n",
        "\n",
        "\n",
        "# Define Siamese Network\n",
        "class SiameseNetwork(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # Define convolutional layers for feature extraction\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=0)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=0)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
        "        self.fc1 = nn.Linear(230400, 256)  # Assuming input image size is 64x64\n",
        "\n",
        "    def forward(self, x): #prima forward_once\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
        "\n",
        "        x_flat = torch.flatten(x, 1)  # Appiattisce il tensore, mantenendo la dimensione del batch\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "    # def forward(self, input1, input2):\n",
        "    #     output1 = self.forward_once(input1)\n",
        "    #     output2 = self.forward_once(input2)\n",
        "    #     return output1, output2\n",
        "\n",
        "class WeightedSoftMarginTripletLoss(pl.LightningModule):\n",
        "    def __init__(self,ImagePairDataset, loss_weight=10.0):\n",
        "        super(WeightedSoftMarginTripletLoss, self).__init__()\n",
        "\n",
        "        self.ImagePairDataset = ImagePairDataset\n",
        "        # Define the sizes for train, validation, and test sets\n",
        "        train_size = int(0.7 * len(ImagePairDataset))  # 70% for training\n",
        "\n",
        "        val_size = int(0.15 * len(ImagePairDataset))   # 15% for validation\n",
        "\n",
        "        test_size = len(ImagePairDataset) - train_size - val_size  # Remaining for testing\n",
        "\n",
        "        # Use random_split to split the dataset\n",
        "        self.datatrain, self.dataval, self.datatest = random_split(ImagePairDataset, [train_size, val_size, test_size])\n",
        "\n",
        "        self.SiameseNetwork = SiameseNetwork()\n",
        "        self.loss_weight = loss_weight\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        grd_global = self.SiameseNetwork.forward(x1)\n",
        "        sat_global = self.SiameseNetwork.forward(x2)\n",
        "\n",
        "        dist_matrix = 2 - 2 * torch.matmul(sat_global, grd_global.t())\n",
        "\n",
        "        pos_dist = torch.diag(dist_matrix)\n",
        "\n",
        "        batch_size = sat_global.size(0)\n",
        "\n",
        "        pair_n = batch_size * (batch_size - 1.0)\n",
        "\n",
        "        # ground to satellite\n",
        "        triplet_dist_g2s = pos_dist - dist_matrix\n",
        "\n",
        "        # satellite to ground\n",
        "        triplet_dist_s2g = pos_dist.unsqueeze(1) - dist_matrix\n",
        "\n",
        "\n",
        "        return triplet_dist_g2s , triplet_dist_s2g , pair_n ,grd_global, sat_global\n",
        "\n",
        "\n",
        "    def loss(self,triplet_dist_g2s , triplet_dist_s2g , pair_n ):\n",
        "        loss_g2s = torch.sum(torch.log(1 + torch.exp(triplet_dist_g2s * self.loss_weight))) / pair_n\n",
        "        loss_s2g = torch.sum(torch.log(1 + torch.exp(triplet_dist_s2g * self.loss_weight))) / pair_n\n",
        "        loss = (loss_g2s + loss_s2g) / 2.0\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validate_topk(self, dist_array, topK):\n",
        "        accuracy = 0.0\n",
        "        data_amount = 0.0\n",
        "\n",
        "        for i in range(dist_array.shape[0]):\n",
        "            gt_dist = dist_array[i, i]\n",
        "            prediction = np.sum(dist_array[i, :] < gt_dist)\n",
        "            if prediction < topK:\n",
        "                accuracy += 1.0\n",
        "            data_amount += 1.0\n",
        "        accuracy /= data_amount\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def validate(self, grd_descriptor, sat_descriptor):\n",
        "        accuracy = 0.0\n",
        "        data_amount = 0.0\n",
        "        dist_array = 2 - 2 * np.matmul(sat_descriptor.cpu(), np.transpose(grd_descriptor.cpu()))\n",
        "\n",
        "        top1_percent = int(dist_array.shape[0] * 0.01) + 1\n",
        "        for i in range(dist_array.shape[0]):\n",
        "            gt_dist = dist_array[i, i]\n",
        "            prediction = torch.sum(dist_array[:, i].cpu().lt(gt_dist.cpu()))\n",
        "            if prediction < top1_percent:\n",
        "                accuracy += 1.0\n",
        "            data_amount += 1.0\n",
        "        accuracy /= data_amount\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      grd, sat = batch\n",
        "      dist_g2s , dist_s2g , pair_n , grd_global , sat_global = self.forward(grd ,sat)\n",
        "      loss_val = self.loss(dist_g2s , dist_s2g , pair_n)\n",
        "      self.log('train_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "      tqdm_dict = OrderedDict({\"loss_train\": loss_val})\n",
        "      output = {\"loss\": loss_val, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
        "      return output\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        grd, sat = batch\n",
        "        dist_g2s, dist_s2g, pair_n, grd_global , sat_global = self.forward(grd, sat)\n",
        "        loss_val = self.loss(dist_g2s, dist_s2g, pair_n)\n",
        "        accuracy_val = self.validate(grd_global, sat_global)  # Calcola l'accuratezza\n",
        "        self.log('val_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        self.log('val_accuracy', accuracy_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        output= {\"val_loss\": loss_val, \"val_accuracy\": accuracy_val}\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        grd, sat = batch\n",
        "        dist_g2s, dist_s2g, pair_n, grd_global, sat_global = self.forward(grd, sat)\n",
        "        loss_val = self.loss(dist_g2s, dist_s2g, pair_n)\n",
        "        accuracy_val = self.validate(grd_global, sat_global)  # Calcola l'accuratezza\n",
        "        self.log('test_loss', loss_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        self.log('test_accuracy', accuracy_val, on_step=False, on_epoch=True, prog_bar=True )\n",
        "        return {\"test_loss\": loss_val, \"test_accuracy\": accuracy_val}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        return whatever optimizers we want here\n",
        "        :return: list of optimizers\n",
        "        \"\"\"\n",
        "        optimizer = optim.SGD(self.parameters(),\n",
        "                             lr=0.01, momentum=0.90)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                         T_max=10)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def __dataloader(self, train, dataset):\n",
        "        # when using multi-node (ddp) we need to add the  datasampler\n",
        "        train_sampler = None\n",
        "        batch_size = 80\n",
        "\n",
        "        should_shuffle = train and train_sampler is None\n",
        "        loader = DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=should_shuffle,\n",
        "            sampler=train_sampler,\n",
        "            num_workers=0,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "        return loader\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        logging.info('training data loader called')\n",
        "        return self.__dataloader(train=True, dataset=self.datatrain)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        logging.info('val data loader called')\n",
        "        return self.__dataloader(train=False, dataset=self.dataval)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        logging.info('val data loader called')\n",
        "        return self.__dataloader(train=False, dataset=self.datatest)\n",
        "\n"
      ],
      "metadata": {
        "id": "orydrr8-mkG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data setup\n",
        "# Assuming you have ground_images and aerial_images as lists of image paths\n",
        "# Assuming you have defined appropriate transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "# Create a RichProgressBar callback\n",
        "progress_bar = RichProgressBar()\n",
        "\n",
        "# Initialize dataset and dataloader\n",
        "dataset = ImagePairDataset(ground_images, aerial_images, transform=transform)\n",
        "\n",
        "# Initialize Siamese Network, Triplet Loss, and LightningModule\n",
        "\n",
        "model = WeightedSoftMarginTripletLoss(dataset)\n",
        "\n",
        "trainer = Trainer(max_epochs=20 , callbacks=[progress_bar])\n",
        "\n",
        "trainer.fit(model)\n"
      ],
      "metadata": {
        "id": "U80tvd8QclPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(model)"
      ],
      "metadata": {
        "id": "6ZtjHh4Evvfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=logs\n"
      ],
      "metadata": {
        "id": "tn-K6-hxw6gn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FTH52kmbCo0d",
        "wdMTOVNGOdLR",
        "n8LvyWcZT6Sy",
        "YHv90khTDZ6D",
        "jOInv_qCa9c4",
        "1W_LVSIQpRBJ",
        "zhmuKdXDbNvn",
        "Klmeyr0iD0D2"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5292d1d30d3441a8feba1c4303932d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0a762484a71477b8194921a7e9c9adb",
              "IPY_MODEL_26080088f6bc45e8be22d49f3aca300c",
              "IPY_MODEL_cd0d39b103fa465784b9d7a2ee0eb650"
            ],
            "layout": "IPY_MODEL_4835c22f40574b17a88c128fcbb0e6bf"
          }
        },
        "f0a762484a71477b8194921a7e9c9adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bf53a74fff949c6b04414fe6e530a4d",
            "placeholder": "​",
            "style": "IPY_MODEL_6eeb98aaee8247dd907952d052598793",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "26080088f6bc45e8be22d49f3aca300c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_918a00e4b8c04d5ea4681b3c877a73fc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fae79de488024e2c8b305f85779aa08d",
            "value": 2
          }
        },
        "cd0d39b103fa465784b9d7a2ee0eb650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d216f9f882c405fb81c069dc2bd0687",
            "placeholder": "​",
            "style": "IPY_MODEL_d4651bb156104b29a213464cc1cd4143",
            "value": " 2/2 [00:00&lt;00:00,  7.14it/s]"
          }
        },
        "4835c22f40574b17a88c128fcbb0e6bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "4bf53a74fff949c6b04414fe6e530a4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eeb98aaee8247dd907952d052598793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "918a00e4b8c04d5ea4681b3c877a73fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fae79de488024e2c8b305f85779aa08d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d216f9f882c405fb81c069dc2bd0687": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4651bb156104b29a213464cc1cd4143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e98c2e05772e4863b73365128a9aa15b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b69639b4b33a4c88b0c2cd537211056c",
              "IPY_MODEL_c60798683684407c837ceddeb52cc7c5",
              "IPY_MODEL_481edc1378154bc08dcdebdc99c56f42"
            ],
            "layout": "IPY_MODEL_ae537f70181e44aaaeb8199facb96db8"
          }
        },
        "b69639b4b33a4c88b0c2cd537211056c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95ed057cd7d84c8dbf62d6a47c0d6189",
            "placeholder": "​",
            "style": "IPY_MODEL_82f2187e09844187baf4dedb8ca2a7d8",
            "value": "Epoch 0:   0%"
          }
        },
        "c60798683684407c837ceddeb52cc7c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60340a0eeca144efb830825f012f2de4",
            "max": 887,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12be7ead9570488994a6a4b52221cc17",
            "value": 0
          }
        },
        "481edc1378154bc08dcdebdc99c56f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54c36d5c1fe14e9dbda34b28519bbe95",
            "placeholder": "​",
            "style": "IPY_MODEL_9441d7e850b84e64bec1cb235797d98c",
            "value": " 0/887 [00:00&lt;?, ?it/s]"
          }
        },
        "ae537f70181e44aaaeb8199facb96db8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "95ed057cd7d84c8dbf62d6a47c0d6189": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f2187e09844187baf4dedb8ca2a7d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60340a0eeca144efb830825f012f2de4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12be7ead9570488994a6a4b52221cc17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54c36d5c1fe14e9dbda34b28519bbe95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9441d7e850b84e64bec1cb235797d98c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}